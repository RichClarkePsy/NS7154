[["index.html", "NS7154 handbook Chapter 1 Welcome", " NS7154 handbook Richard Clarke 11/08/2023 Chapter 1 Welcome Welcome to the R Handbook for NS7514 Health in Context: From Cell to Society Over the course of the semester this book will guide you through the use of R and RStudio to organise, visualise, and analyse health data related to the lecture content from each week of this module. This book will take you from the absolute basics of R, to the point where you are able to make stunning visualisations, expert analysis and, most importantly, conduct the analysis for your assignment. Week 1 - Introduction to module / Introduction to R Week 2 - Global burden of disease / Basic data visualisation Week 3 - Genetics &amp; epigenetics / Working with data files Week 4 - Biological systems involved in health / Working with categorical data Week 5 - Psychobio interaction / Simple linear regression Week 6 - Lab based and biometric data collection methods / Catch up week Week 7 - What is public health? / Multiple regression Week 8 - Applied public health / Logistic regression Week 9 - Social determinants of health / Assignment data overview Week 10 - Planetary health / Introduction to RMarkdown Week 11 &amp; 12 - Biopsychosocial connections / Working on your own analysis "],["week-1---why-learn-r.html", "Chapter 2 Week 1 - Why learn R?", " Chapter 2 Week 1 - Why learn R? If you've completed an undergraduate psychology degree there's a good chance that you've been taught statistical analysis using SPSS or STATA. These programs are great, and are capable of conducting some amazing analysis, but data in the real world is often a lot messier than the data you saw during your undergraduate degree. I want you to leave this MSc with the skills needed to deal with that messiness, wherever you may end up, and for me R is an excellent place to hone those skills. Need more convincing? Here are the usual go-to reasons for learning R as a psychology student. 1. R is open access and therefore free SPSS and STATA are ludicrously expensive. A personal licence, at my last check, cost a little over $1000 per year. This is fine for a university to cover (for now), but move away from academia to perhaps a charity or a non-governmental organisation and it's a cost that many are not willing to pay for. As such, you may find that there are many jobs out there that you are perfectly capable of doing, from a knowledge point of view, but they just don't have access to the tools you've been trained on. This was the position that I was in after I finished my last post-doc. Doors were closed to me - really interesting doors where I could have done a lot of good - due to my skill base being too narrow to be applicable to them. I don't want that to be the case for you. By the end of this course you'll be able to do all the analysis you can already do on the paid software, in this free and accessible language that is quickly becoming a highly desirable skill for a graduate to have. 2. Data wrangling Say you want to conduct some longitudinal research. You find participants, you send them surveys each month, you also perhaps visit them in person and take some bio-physiological measurements (blood pressure, BMI etc). All of this data is recorded and saved across multiple different excel files. Some participants drop out or fail to respond for some of the time points. How do you match participants for your final dataset and combine files into a format where you're able to run your analysis? This is a problem that we very rarely cover at undergraduate, but it's a very common and essential part of the research process, especially with health data collected by a third party (i.e. a charity or the NHS). In the days before I learnt R this kind of problem led me to copy and pasting chunks of data in excel, entering data into SPSS by hand, or any number of long laborious processes that were prone to silly mistakes during lapses of concentration or poor work processes. There has to be an easier way! Good news, there is, and it's R! The skill to cope with the data problem I've described above, and many others, is known as data wrangling and it's a skill that is best deployed with code. By the end of this module, you should be able to have all your raw data for a project together in a folder, open a script you have written, click run and end up with a perfect dataset ready for your analysis every time. Image from R for data science. It is explained in more detail here 3. Data visulisations I never truly understand my data until I can see it. Relationships between variables, differences between groups, location data, all can be visulised quickly and effectively using R. Here are a few of my favourites that you will be creating across the course of the semester: And the great thing about creating these with code is that next time you come across similar data in the same format you can simply copy your previous code and adapt it to your needs. No more clicking around in excel trying to find the same colours and arrangement from last time, no more ugly SPSS outputs, just publication ready figures that communicate your amazing findings in a way the words just can't capture. 4. Reproducability If you've been in psychology for a while you may have heard that we are in a \"Replication Crisis\". Findings that were previously held up as reliable understandings of the human condition are crumbling into dust once we apply the rigour of the scientific method. A, now infamous, 2015 study The Open Science Collaboration followed the methodology of 100 experimental reports, from a single year (2008), that had been published in a range of high-ranking journals. The aim was to see how many of the findings replicated (i.e. when repeated, produced the same results). Out of the 100 only 39 were found to successful replicate. This finding proved a watershed moment in psychology whereby many of us were starting to question the reliability of our foundational work. One solution, suggested by the academic community, is to make the process of our science far more transparent. This is were R comes in. The use of R by no means solves all the issues with the replication crisis, but by submitting code along with making our data open access, readers of our work no longer need to \"take our word for it\" that we've conducted an appropriate analysis and correctly reported our results. They can now check for themselves. Journals are also starting to take note of such a culture shift and are starting to require the submission of analysis scripts. So, with any luck, this skill will start to align more with you getting published in the future. Hopefully I've convinced you of the importance of learning R as a psychology student. If not here is a link to particularly good talk by Phil McAleer from the University of Glasgow on why they moved their undergraduate degree towards a focus on reproducible research, using R for statistics. Now on with the learning! "],["week-1---getting-started.html", "Chapter 3 Week 1 - Getting started 3.1 Installing R and RStudio 3.2 Opening RStudio for the first time 3.3 Packages", " Chapter 3 Week 1 - Getting started 3.1 Installing R and RStudio R is a programming language that you’ll be learning to write code in, while RStudio is what is known as an Integrated Development Environment (IDE) which makes working with R easier. I will largely use the terms \"R\" and \"RStudio\" interchangeably throughout this course, but you can think of writing in R as you would writing in English and RStudio the program you write it in. You could write all your written assignment in NotePad but Microsoft Word gives you a lot more freedom so that’s often the default. The same logic is true for R, using RStudio just makes life a lot easier. To get started you will first need to install the language R and then the IDE of RStudio. Both can be installed, for free, from the company that maintains them, Posit. From there you will then need to select your operating system (Windows or Mac). Click here for the Posit R and RStudio download page One of the best things about using R and RStudio is that they are both very widely used across many academic disciplines, and in the professional world. As such there are many guides for installation. If you hit on an issue with installation, it's very likely others have too. Here are some helpful links to help you install R and RStudio your own home computer (which I highly recommend you do): University of Glasgow PsyTeachR installation guide YouTube video showing the download process for Windows YouTube video showing the download process for Mac 3.2 Opening RStudio for the first time When you first open RStudio you should be greeted by a page that looks like this: If not double check that it is RStudio that you are opening and not just R in a text editor. First thing I suggest you do is click File -&gt; New File -&gt; R Script. This will show you the normal view you will get when working in R. Here you have four segments to your screen: The console (Bottom left) - For directly running short snippets of code, also where all your output and errors (sooo many errors) will be shown. The script editor (Top left) - For writing longer chunks of code that you'll then be able to save as a file. An area that shows you any functions and objects you create (Top right). An area that shows you any of the fancy figures you create, also somewhere that you can navigate your files from (Bottom right). You’ll become intimately aware of what each of these areas do over the course of this module but if you want a overview of these sections now this video give a good introduction to each section (I recommend watching from around 5mins 45sec). Finally, for the basic set up, it is very important to look cool while doing your analysis in R. To change the basic appearance: Click Tools -&gt; Global Options -&gt; Appearance. I like the Editor Theme Vibrant Ink. It makes me feel like I'm a hacker from the 90's. Also, if you're getting old like me you may want to make use of View -&gt; Zoom in option to increase the size of the text. Stressed yet? Don't worry, we'll have your screen looking like this in no time. There is a learning curve for R but once things start to click you'll love it! 3.3 Packages The basic functions of R are great, but one of the most useful things about working in R (instead of the commercial software) is its ability to incorporate user generated content to customise what we're able to do. Such add-on content are known as packages. The validated and most frequently used packages are stored on the CRAN (Comprehensive R Archive Network) server and RStudio knows exactly where to find them, as long as you know the name of the package. So no need to click around random websites downloading files to install, which is nice. For this module we will make extensive use of the tidyverse package. This is a package full of the most frequently used packages in R coding, bringing together a set of grammar and visulisation tools that makes the process of writing in R a whole lot easier. To install the tidyverse package type the following code into your console (next to the bottom &gt;) and then hit the enter key to run the code. Or you can copy and paste directly from this document by clicking the clipboard icon in the top right of the box. install.packages(&quot;tidyverse&quot;) You only ever need to install a package once per computer but you will need to load it each time you open RStudio. In these subsequent times this is the only code you'll need: library(tidyverse) For each analysis that you conduct it's a good idea to include the packages as the first code on the script, so copy out this code and place it at the top of your new script (top left of your screen, if you've created one). To run this, and any code, you can either highlight the code and click the Run button in the top right of the script window. Or you can navigate to the line that you want to run and hit Ctrl+Enter. "],["week-1---writing-and-running-your-first-code.html", "Chapter 4 Week 1 - Writing and running your first code 4.1 R as an overpowered calculator 4.2 This is a \"smart book\" 4.3 Meet your pesonal R tutor ChatGPT (or alternitive) 4.4 Back to the coding 4.5 Video walkthrough 4.6 Test yourself exercises", " Chapter 4 Week 1 - Writing and running your first code Okay, lets get you coding! 4.1 R as an overpowered calculator Firstly, R is an excellent calculator. Take the following lines of code and try running them in the console or from your script window. 1+1 4*(14-4) 10^2 sqrt(144) Reminder of how to run code To run code in the console (the bottom left window of RStudio) click on the bottom &gt; symbol, type or paste the code you would like to run, and then hit the Enter key. To run code from your script window: Make sure you have a script window open, you can open a new script by clicking File -&gt; New File -&gt; R Script. Type or paste the code you want to run. Highlight your code and click Run or navigate to the line/chunk you would like to run and hit Ctrl+Enter (for a mac it's Command+Enter) After running each line of code the output is reported in the console and should look something like this: ## [1] 2 ## [1] 40 ## [1] 100 ## [1] 12 What does the [1] mean? The [1] at the beginning of each output is a way of indexing where you are in a output. Take the following code for instance: rnorm(50, 5, 1) Running this generates a set of 50 data points that are normally distributed with a mean of 5 and a standard deviation of 1. As it creates a long output that spans over multiple lines the number in the [] gives you an indication of how many data points are displayed on each line. The final line of code that you ran used a function, in this case the sqrt function that is used in order to perform a square root calculation. When typing it into RStudio you may have noticed that after typing the first three letters a box popped up with the rest of the function and a short explanation of the function. This is a super helpful feature of RStudio that you will end up using a lot. The {base} indicates that this function exists within the base version of R. Later we will be using functions that we've added with the tidyverse package. Its worth keeping an eye on these as a common error is trying to use a function from a package that has not been loaded. Another way to learn about a function is by running: ?sqrt. This loads a help file in the bottom right that explains more information about the function. Try running the following for functions from the dplyr package that is included in the tidyverse package: library(tidyverse) ?mutate ?select ?arrange Click here if you received an error message If you received an error message that reads somthing like this: No documentation for ‘mutate’ in specified packages and libraries: you could try ‘??mutate’ Then this is likely due to you not having yet installed or loaded the tidyverse package. To install simple run install.packages(\"tidyverse\") in your console or from your script. Read the previous chapter that covers packages for more detail on this. These help files are likely to be confusing for you at the moment but they will make a lot more sense the further you go with your coding. 4.2 This is a \"smart book\" Did you see the blue boxes in the section above? I will be using these throughout this book to include additional explanation or hide solutions to exercises so you can first try to work out the code for yourself. Also in some places I will include questions that allow you to self-test your knowledge. For example: The square root of 36864 is Hint Use the sqrt() function The box will turn green when you input the correct number. What code would you use to load the help file for the rnorm function? Solution ?rnorm Is using AI permitted on this module? Absolutely not, how dare you!Yes, in fact it's encouraged! 4.3 Meet your pesonal R tutor ChatGPT (or alternitive) Well this is as good a time as any to introduce you to my teaching assistant for this module, ChatGPT (or an equivalent alternative) Writing code has become vastly easier in the last year with the public release of various Large Language Models (LLMs) AIs. I won't go into depth with how LLMs work (mainly because I don't actually know) but in very basic terms a LLM is an artificial intelligence that has been \"trained\" on an unfathomably large amount of language data from across the internet. As I've previously mentioned, many academic disciplines and businesses use R for data analyses and as such there are many guides and resources online explaining how to write code in R. Talking to an LLM is like talking to someone that has read, memorised, and \"understands\" all of these guides and is able to explain pretty much most of it in a very simple to understand way. Personally, I use a LLM called ChatGPT from OpenAI, you can set up a free account with just an email address. There is a paid version, but you shouldn't need that for what we'll be using it for this semester. Lately, I've been working with the AI to write apps for teaching undergraduate statistics (like this) and I have to say it makes learning new aspects of coding far quicker and more enjoyable than hacking my way through Google or YouTube looking for a suitable explanations. However, it is by no means perfect. Its biggest limitation is currently that it is only trained on data up until 2021. Meaning that it can’t advise on any changes to R (or packages) since the end of its training. Also, if it doesn’t know the answer it will just make stuff up and confidently present it as fact (it’s a lot like me in that respect!). So, it's best to never trust it completely. As it's a chatbot I thought I'd be nice to let ChatGPT introduce themselves, click this link to see the transcript of our conversation and a demonstration of what working with an AI can do for your R learning. Click here for the full conversation Seeing as we've only just entered this brave new world of AI, for now, I would ask you to only use LLMs as an aid for your R coding rather than your written work (no doubt some of you have heard about its hugely disruptive ability to write essays). And at this stage please only use it for my module, your other lecturers may have different preferences which I would like you to respect. If in doubt, ask. 4.4 Back to the coding If you're only using R as a basic calculator you're basically using a sledgehammer to crack a walnut. Obviously coding with R can do so much more. One really important aspect of R is assigning data to a object. 4.4.1 Assigning numbers to objects Run the following code line by line. Each time you run a line take a look in the environment tab in the top right window of RStudio. a &lt;- 5 b &lt;- 10 c &lt;- a+b a+b+c The first two lines use the &lt;- notation (which is made up of the less than &lt; and dash -symbols) to assign a number to a letter. The third line adds the value of a to the value of b and creates a new object of c containing the value of the sum of a+b. The fourth line add each of the three created objects together, if you look in the console you'll see the output of the sum. ## [1] 30 Here's another example, this time based on this classic question popularised in Daniel Kahneman's work on system 1 and system 2. A question where your initial gut reaction is reliably likely to give you the wrong answer, but we can use code to help us work it out. Question: A bat a ball cost £1.10, The bat costs £1.00 more than the ball. How much does the ball cost? You're first instinct is likely to say that the ball costs £0.10 but the answer is actually £0.05. And we can check this with what we've just learnt about assigning values to objects. See if you can adapt the code below appropriately to test this. Reassigning a value to a object overrides the previous value, or you can click the little broom icon in the environment window to remove all previous objects. # change out the question mark until you get the correct answer of 1.1 Ball &lt;- ? Bat &lt;- Ball + 1.00 Bat + Ball Click if you're intrested in the algebre Don't worry this question still hurts my brain, even after teaching it for years! Bat + Ball = 1.10 Bat = 1.00 + Ball 1.00 + 2*Ball = 1.10 Ball = (1.10 - 1.00)/2 Ball = 0.1 / 2 Ball = 0.05 4.4.2 Assigning multiple data point to an object An object can hold more than just a single value. In the following code a range of numerical data is being created, and is assigned to the object age. The c() is a function that concatenates (fancy word for links) values together and again the &lt;- stores this to a single object, in this case age. After assigning this data to age if your run age (either in the console or from script) then you the list will appear as output in your console. age &lt;- c(23, 57, 42, 12, 8, 92, 35, 86, 26, 65) Sometimes the data we want is just a list of consecutive numbers. The following lines of code are equivalent however one is substantially quicker than the other, especially at scale. ID_number &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) ID_number &lt;- 1:10 We can also assign words/letters (known as string data) to an object. names &lt;- c(&quot;Kayleigh&quot;, &quot;Lisa&quot;, &quot;Beatrice&quot;, &quot;Jessie&quot;, &quot;Hugo&quot;, &quot;Justin&quot;, &quot;Mohammed&quot;, &quot;Shawn&quot;, &quot;Hasan&quot;, &quot;Kelly&quot;) gender &lt;- c(&quot;Female&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Non-binary&quot;) Some of you might see where we're going with this, and yes we can combine these objects into a dataset and assign it to yet another object. data &lt;- data.frame(ID_number, age, names, gender) view(data) From this you can see that we've created a new type of object in our environment window, a Data object. It tells us how many observations there are (in this case, rows of data) and how many variables there are (in this case, columns of data). We can now view this data by clicking on it or by using the view() command. 4.5 Video walkthrough Here is a video of me talking through some of the basics and working through the above content: 4.6 Test yourself exercises 4.6.1 Exercise 1 Create a dataframe that contains the data from the following table: Solution This is not the only way to do this, if you've achieved the same results a different way. Great, well done! Age &lt;- c(18, 18, 19, 22, 24, 24, 25, 29, 35, 42, 52, 68) ID &lt;- 1:12 Gender &lt;- c(&quot;Female&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;,&quot;Female&quot;,&quot;Non-binary&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;,&quot;Non-binary&quot;) Score &lt;- c(100, 89, 92, 62, 100, 75, 78, 89, 100, 86, 68, 85) testscore &lt;- data.frame(ID, Age, Gender, Score) 4.6.2 Exercise 2 Next use the rnorm function to generate a variable called IQ that includes a data point for each participant in our dataset (e.g. whatever our N equals). Mean should equal 100 and you can choose a realistic standard deviation. Hint R is case sensitive so double check that you've the same capatalisation as your objects throughout. Don't forget you can use the ?rnorm command to open the help file for the rnorm function. This will show you how to use it. Solution Again, this is not the only way to do this, if you've achieved the same results a different way. Great, well done! IQ &lt;- rnorm(12, 100, 15) testscore &lt;- data.frame(ID, Age, Gender, Score, IQ) 4.6.3 Exercise 3 A cup of tea and a biscuit together cost £1.50. The cup of tea costs £1.30 more than the biscuit. How much does the biscuit cost? Solution Tea + Biscuit = 1.50 Tea = 1.30 + Biscuit Biscuit &lt;- 0.1 Tea &lt;- Biscuit + 1.30 Tea + Biscuit Again, don't worry, this is meant to be a counterintuitive question. This book is a work in progress. If you have any feedback or spot any mistakes please enter them into this feedback survey quoting the week your comment is referring to. Thank you! :-) "],["week-2---basic-visualisation.html", "Chapter 5 Week 2 - Basic visualisation 5.1 Introduction 5.2 Getting an overview of your data 5.3 Visualising global life expectancy 5.4 Correlation between variables 5.5 Test yourself exersise 3 5.6 Video walkthrough", " Chapter 5 Week 2 - Basic visualisation 5.1 Introduction This week, I want to introduce you to the data visualisation package ggplot2 (which is contained within the tidyverse). ggplot is an incredibly versatile way of creating graphs and has become the standard throughout academic publishing and data journalism. The proliferation of ggplot Here is a link to a post from the BBC talking about how they recently moved to using ggplot for all their data visualisations Soon, you'll be spotting figures everywhere and start saying to yourself \"I bet I could recreate that with ggplot\", and then get sad when you find out they didn't share the data :-( For the following examples and exercises, we'll be using a 2015 country-level summary of World Health Organisation data related to life expectancy (source). Next week, I will show you how to read in your own data files but for now I've given you the code that imports the dataset directly from my GitHub (the web hosting system that I use for this book). # Run this code to get the data for this week library(tidyverse) who_rawdata &lt;- read_csv(&quot;https://raw.githubusercontent.com/RichClarkePsy/Datasets/main/Life%20expectancy%20dataset.csv&quot;) This week you will need the following packages: library(tidyverse) # contains the ggplot2 package library(ggrepel) # used later to add text labels to data points Reminder about packages Remember if you are using any of these packages (e.g. ggrepel) for the first time you will need to run install.packages(\"packagename\"). It is often best to run this in the console just so you're not re-installing the package each time you run your code. Note the quotation marks around the package name. R is quite sensitive to this sort of precision and, as such, attention to detail ends up solving most of the errors you get. Either that or just ask ChatGPT to do it for you. Both completely legitimate ways to code! If you've worked your way through the content from last week, then you'll already have tidyverse installed but you'll likely need to install ggrepel. 5.1.1 Feeling overwhelmed? Throughout this week there is going to be chucks of code where you have little to no clue what is going on. That's ok! One of the best ways to learn R is to just play around with pre-existing code, change parts and see what happens. If you run into errors that you don't understand, or code you want explained, just copy and paste it into a ChatGPT conversation and it will explain it to you (I've included an example of this later). You can even ask follow up questions or ask it to adapt the code to your specifications. Though it does help to know at least a little about what your code is doing, especially later on in the course when we move on to analysis. 5.2 Getting an overview of your data By this point you should have your packages loaded and have a data object in the top right Environment window named who_rawdata. Just by looking at the data object in the environment window we can see two aspects of the dataset; the number of observations and the number of variables. Observations in this case are the individual countries and variables are the various health and social data variables. Click on the dataset or run view(who_rawdata) to take a look at the data. When you do this, a new tab will appear with a view similar to this: Viewing your data like this is good for a quick look at the data, however, in health we are often using datasets with many hundreds of variables and thousands of observations. So we won't be looking at the data directly as much as we might when using SPSS. Instead we want to get into the habit of exploring the dataset through code. The following gives you some basic information about the dataset in the console. You may want to increase the size of the console window and scroll up and down to see the full information. dim(who_rawdata) str(who_rawdata) summary(who_rawdata) From viewing the output of the above code... What is the variable name for life expectancy? Name a variable that can have letters or numbers as data. This is known as character (chr) data What is the average number of years of schooling across all countries? 5.3 Visualising global life expectancy Statistics are great (and if you've done the undergraduate here you know I love them!), but often, when we want to get a quick and easy understanding of a dataset, we can get everything we want from simple data visualisation. 5.3.1 Histograms Histograms are a good first step towards understanding a continuous variable like life expectancy. To create a histogram we're going to take our dataset then we're going to pipe %&gt;% it in to the ggplot function. Extra details on pipes %&gt;% A pipe is basically a way of saying \"and then\" in code. It helps tidy things up so that we're not creating object after object. So in this case we're saying: take the data and then use that data with the following ggplot expression. This can all be done in a single line of code, however, its a lot neater to hit enter after each pipe and start the next expression on the next line. It also indents for ease of navigation. This is part of the grammar of coding that you'll get use to in time. You'll see the code for pipes like this %&gt;% through this module but this |&gt; also does the same job. The difference is far too nerdy for any of us to care about right now, but just know that they both do basically the same thing. For most of this book you can just adapt the code I have given you, and you'll get more used to the notation like this in time. Each ggplot visualisation is made up of layers. You start by mapping an aesthetic (aes), which involves the organisation of which data goes on which axis and how data is segmented within the visualisation. Then you layer on (using a + sign) a geometry to combine the data with a desired visual format. The following example takes our data, then places life expectancy on the x-axis, and uses the histogram geometry. who_rawdata %&gt;% ggplot(aes(x=Life_exp)) + geom_histogram() This histogram is fine for a quick exploration of the data, but we could also tidy the figure up to the point at which it would be perfectly fine to include it in a paper. who_rawdata %&gt;% ggplot(aes(x=Life_exp)) + geom_histogram(binwidth = 1, colour = &quot;black&quot;, fill = &quot;#58A139&quot;) + labs(title = &quot;Figure 1. Histogram of Global Life Expectancy&quot;, subtitle = &quot;N = 183 countries&quot;, x = &quot;Life expectancy from birth&quot;, y = &quot;Number of countries&quot;, caption = &quot;Year: 2015 | Data source: WHO&quot;) + theme_light() + theme(plot.caption = element_text(hjust = 0.5)) 5.3.2 Test yourself exercise 1 Take the above code and adapt it to work out what each of the arguments (e.g. \"binwidth\" or \"subtitle\") control. Colours can be in words or in hex codes. Here's a useful website where you can get hex codes www.htmlcolorcodes.com. None of this is magic btw I don't keep all of this in my head at all times. At some point or other I've Googled how to do pretty much every element here. Try searching online for the following and see if you can find similar looking code: \"Change the colour of histogram bars in ggplot\" \"Add a caption to a ggplot figure\" \"Align caption to center in ggplot\" Also here's a conversation with ChatGPT where it makes some changes to this final version and then suggests some genuinely helpful additions (dotted gridlines are a nice touch). 5.3.3 Facet wrap This is a good starting visualisation, but say we want to understand the distribution for each region. To do this all we need to do is add the last line of the code below to create a nicely formatted version of the six histograms together. who_rawdata %&gt;% ggplot(aes(x=Life_exp)) + geom_histogram(binwidth = 1, colour = &quot;black&quot;, fill = &quot;#58A139&quot;) + labs(title = &quot;Figure 1. Histogram of Global Life Expectancy&quot;, subtitle = &quot;N = 183 countries&quot;, x = &quot;Life expectancy from birth&quot;, y = &quot;Number of countries&quot;, caption = &quot;Year: 2015 | Data source: WHO&quot;) + theme_light() + theme(plot.caption = element_text(hjust = 0.5)) + facet_wrap(~ Continent) Note about ~ The ~ is often used in R to denote a relationship. You'll often see it in regression analysis but in this case it's telling ggplot to facet (i.e. seperate) the graph by the variable Continent. 5.3.4 Boxplots The above histograms definitely have value (for instance we can now see the difference in life expectancy between Europe and Africa very clearly), but a better way to visualise the same data might be to change from a histogram to a boxplot. who_rawdata %&gt;% ggplot(aes(x=Life_exp, y=Continent)) + geom_boxplot() + theme_light() Much better, instantly we can see how the distribution of life expectancy differs across continents. A few things spring to mind to make this figure more informative. 1. Label the outliers (it'd be nice to know what countries they are). 2. Add some colour. 3. Add a title and label the x-axis. The first point is genuinely quite hard to solve and involves the two lines of code below. The first line creates a new function that can be used to identify outliers in a variable. Then, in the next line, I use the function to make a new variable where evertime the function finds an outlier in Life_exp it assigns the text from the Country variable. findoutlier &lt;- function(x) { return(x &lt; quantile(x, .25) - 1.5*IQR(x) | x &gt; quantile(x, .75) + 1.5*IQR(x)) } who_with_outliers &lt;- who_rawdata %&gt;% group_by(Continent) %&gt;% mutate(outlier_LE = ifelse(findoutlier(Life_exp), Country, NA)) Also, because I've made a change to the data (in this case added a new variable) I've assigned it to a new object name, as I always try to keep the raw data in the same form as it was read in. Just in case I need to use it again later. Notice that you should have a new dataframe object in the environment window after running this. To add these outlines to the figure I've added a layer of text above the box plot layer. For points two and three I've selected a pre-existing colour pallet and mapped it to our Continent variable and I've added labels with the labs function. library(ggrepel) # you&#39;ll need to install this if you&#39;ve not already done so who_with_outliers %&gt;% ggplot(aes(x=Life_exp, y=Continent, fill = Continent)) + geom_boxplot(width = 0.5, alpha = 0.75, show.legend = FALSE) + geom_text_repel(aes(label=outlier_LE), position = &quot;identity&quot;, size = 3, na.rm=TRUE) + scale_fill_brewer(palette = &quot;Dark2&quot;) + labs(title = &quot;Figure 2. Boxplot of Global Life Expectancy&quot;, subtitle = &quot;N = 183 countries&quot;, x = &quot;Life expectancy from birth&quot;, y = &quot;&quot;, caption = &quot;Year: 2015 | Data source: WHO&quot;) + theme_light() + theme(plot.caption = element_text(hjust = 0)) Now that is a good looking figure! There's a lot going on to make these figures but luckily you don't need to keep it all in your head at one time. I often go back to old code or code from webbooks like this or PsyTeachR and adapt and change it for my needs. In fact lets adapt this code for BMI instead of life expectancy. 5.3.5 Test yourself exercise 2 Adapt the code above to make a histogram and boxplot for the BMI variable instead of life expectancy. Note: Here is the code for calculating the outliers for BMI, as this requires adding in the extra line drop_na(BMI). This tells R to remove any missing data in the variable BMI. Note the change in dataset name as well. findoutlier &lt;- function(x) { return(x &lt; quantile(x, .25) - 1.5*IQR(x) | x &gt; quantile(x, .75) + 1.5*IQR(x)) } who_BMI &lt;- who_rawdata %&gt;% drop_na(BMI) %&gt;% group_by(Continent) %&gt;% mutate(outlier_BMI = ifelse(findoutlier(BMI), Country, NA)) Hint After running the code above you'll need to start with the data_BMI file rather than the rawdata. Look to see all the parts of the code that say Life_exp and change these out for BMI Make sure you remember to change the title and axes labels. Solution This is one way the histogram could look: who_BMI %&gt;% ggplot(aes(x=BMI)) + geom_histogram(binwidth = 1, colour = &quot;black&quot;, fill = &quot;#58A139&quot;) + labs(title = &quot;Figure 1. Histogram of Body Mass Index&quot;, subtitle = &quot;N = 183 countries&quot;, x = &quot;BMI&quot;, y = &quot;Number of countries&quot;, caption = &quot;Year: 2015 | Data source: WHO&quot;) + theme_light() + theme(plot.caption = element_text(hjust = 0.5)) And this is one way the boxplot could look: who_BMI %&gt;% ggplot(aes(x=BMI, y=Continent, fill = Continent)) + geom_boxplot(width = 0.5, alpha = 0.75, show.legend = FALSE) + geom_text_repel(aes(label=outlier_BMI), position = &quot;identity&quot;, size = 3, na.rm=TRUE) + scale_fill_brewer(palette = &quot;Dark2&quot;) + labs(title = &quot;Figure 3. Boxplot of global BMI&quot;, subtitle = &quot;N = 181 countries&quot;, x = &quot;BMI&quot;, y = &quot;&quot;, caption = &quot;Year: 2015 | Data source: WHO&quot;) + theme_light() + theme(plot.caption = element_text(hjust = 0)) What do you notice about the data from this visual check? Personally, I'm concerned about those low BMI data points, BMI should never really be that low. Perhaps this is a data recording issue. No way to know without looking at the individual level data. For further analysis we may want to filter out these data points, or at least check them against other data sources. 5.4 Correlation between variables Next, let's take a look at how good ggplot is at making scatter plots. The following code plots number of years of schooling (on the x-axis) against life expectancy (on the y-axis) for each of our 183 countries. Again, this takes the dataset and pipes it into ggplot. The aesthetic is years of schooling on the x-axis and life expectancy on the y-axis. The geometry layer is using geom_point (rather geom_boxplot or geom_histogram) which creates a scatter plot. who_rawdata %&gt;% ggplot(aes(x = Schooling, y = Life_exp)) + geom_point() That looks to be a very clear correlation to me. In the coming weeks we'll look at strength and statistical significance of such relationships, but for now, to tidy this up, we can add the regression line (by adding an extra geometry layer of geom_smooth), title, and label the axes. who_rawdata %&gt;% ggplot(aes(x = Schooling, y = Life_exp)) + geom_point() + geom_smooth(method = &quot;lm&quot;, formula = y ~ x) + labs(title = &quot;Figure 2. A scatter plot of life expectancy by number of years of schooling&quot;,) + xlab(label = &quot;Average number of years of schooling&quot;)+ ylab(label = &quot;Average life expectancy (in years)&quot;) What happens when you change the order and put the geom_smooth before the geom_point function? Only the dots are visableOnly the line is visableThe dots now appear ontop of the lineEverything breaks! There are usually multiple different ways to achieve what you want to achieve with ggplot. If you delete the bottom two lines of code, can you find a way to label the x and y-axes within the labs function? Hint Check the code from earlier Solution who_rawdata %&gt;% ggplot(aes(x = Schooling, y = Life_exp)) + geom_point() + geom_smooth(method = &quot;lm&quot;, formula = y ~ x) + labs(title = &quot;Figure 2. A scatter plot of life expectancy by number of years of schooling&quot;, x = &quot;Average number of years of schooling&quot;, y = &quot;Average life expectancy (in years)&quot;) 5.4.1 Adding to the aestetic There is one extra thing we can do with the aesthetics part of our code for our scatter plot and that's assign colour or size to our data points. The code below uses the Continent variable to assign colour. who_rawdata %&gt;% ggplot(aes(x = Schooling, y = Life_exp, colour = Continent)) + geom_point() + geom_smooth(aes(group = 1), method = &quot;lm&quot;, formula = y ~ x, colour = &quot;black&quot;) + labs(title = &quot;Figure 3. A scatter plot of life expectancy by number of years of schooling&quot;, x = &quot;Average number of years of schooling&quot;, y = &quot;Average life expectancy (in years)&quot;) # delete aes(group = 1) &amp; ,colour = &quot;black&quot; to see why I included them The code below uses the Population variable to assign size to the data points. Now, the larger the population the larger the dot size. who_rawdata %&gt;% ggplot(aes(x = Schooling, y = Life_exp, colour = Continent, size = Population)) + geom_point() + geom_smooth(aes(group = 1), method = &quot;lm&quot;, formula = y ~ x, colour = &quot;black&quot;) + labs(title = &quot;Figure 3. A scatter plot of life expectancy by number of years of schooling&quot;, x = &quot;Average number of years of schooling&quot;, y = &quot;Average life expectancy (in years)&quot;) + guides(size = FALSE) # delete this line to see why I included it The comments in the code also indicate parts where I had AI assistants. I couldn't remember how to make these changes off the top of my head. Googleing would have taken about 15 mins to find the right guide with the right code but my requests to ChatGPT took less than a minute and also gave an explanation of how the extra code works. This is also a great example of how little the AI needs to work out what you're asking. 5.5 Test yourself exersise 3 The Gapminder organisation is one of my all time favourite non-profits. Their mission is to promote a fact-based worldview that everyone can understand. Never have I seen data about demography and global helath issues communicated in such an engaging way. For instance, here is the best 5 minutes of data communication I've ever seen. Sadly, we lost the amazing Hans Rosling in 2017 but his son and daughter-in-law continue to do incredible work in this area and are worth following. Gapminder have created an R package that contains data similar to the data we've just been using. Install the package, load the package, and import the data using the code below. install.packages(&quot;gapminder&quot;) library(gapminder) gapminder_data &lt;- gapminder::gapminder By looking at the data you can likely see that there's a lot more of it this time. That's due to there also being a year variable. The code below just extracts the data for the most recent year in the dataset, 2007. gapminder_2007 &lt;- gapminder_data %&gt;% filter(year == 2007) Now play with this data! Try to adapt the code from this week and see what you can produce. Here is a list of things you could try: Create histograms for life expectancy and GDP. Create boxplots for life expectancy and GDP. Create a simple scatter plot of life expectancy and GDP. Add the continent and population size data to your scatter plot from 3. Make sure each has an appropriate title and axis. Good luck, and feel free to share what you create on our module teams area. 5.6 Video walkthrough This book is a work in progress. If you have any feedback or spot any mistakes please enter them into this feedback survey quoting the week your comment is referring to. Thank you! :-) "],["week-3---working-with-data-files.html", "Chapter 6 Week 3 - Working with data files 6.1 Introduction 6.2 Reading data into R from a data file 6.3 Data wrangling 6.4 Video walkthrough 6.5 Test yourself exercise", " Chapter 6 Week 3 - Working with data files 6.1 Introduction Our aim this week is to reproduce the figure below from Bravo-Gutierrez et al (2021). The figure shows the percentage of methylation for two genes, AHRR (fig. 2a) and PRSS23 (fig. 2b), as compared across smokers and non-smokers. The genetics aspect might be confusing (at least to us psychologists) but the statistics are very basic (a Mann-Whitney U test). The authors found a significant difference across methylation on the AHRR gene between smokers and non-smokers (p=0.003), with significantly lower levels of methylation for the smokers (indicating the suggested hypomethylation effect of smoking on the AHRR gene), but no significant difference in methylation across the PRSS23 gene (p=0.276). The authors of the paper have made the data from the study open access (as all good scientists should!), but not their code. Let's replicate the analyse and see if we can find the same findings and create the same image. Download the data from the supplemental materials at the bottom of the paper or from this weeks section in the NS7154 Moodle area. For this week your will need the following packages. If needed, check previous weeks for how to install and load packages. library(tidyverse) library(ggpubr) # to add p-values to our graphs 6.2 Reading data into R from a data file Data is often messy, this is especially true of data you don't collect yourself, and it's especially especially true for data that other academics share online. Thankfully, the data shared for this paper is nice and neat and tidy, we only need to take a few steps to get it into R. 6.2.1 Setting your working directory Each time you start an analysis in R I recommend that you create a new folder. Put all the data you want to analyses in that folder. Open a script and save that script in your newly created folder along with your data. Once everything is in the same place, back in RStudio, set your working directory by clicking Session -&gt; Set Working Directory -&gt; Choose Directory or To Source File Location and then select your chosen folder. This allows R to know where to look when you want to import data, and also where to save any outputs. It may seem like a redundant step when we're just making a couple of plots, but as your projects become more complicated this kind of workflow becomes invaluable. 6.2.2 File formats and reading in the data In Excel, open the file downloaded from the supplemental materials of Bravo-Gutierrez et al 2021. You should see four tabs at the bottom of the Excel window. While it is possible to directly read Excel files into RStudio, for the sake of simplicity, we're going to convert the specific tab we're interested into a Comma-Separated Values (CSV) file. A CSV file is simply a text file where the values in our dataset are separated by commas. Our data is on tab S4 and luckily it's in a lovely state and just requires us to navigate to the S4 tab, click save as and then select CSV from the drop down \"save as type\" menu. Save this to the folder you created previously. The nightmare excel file Before continuing, take a quick look at the other tabs in the file. Tab one looks intimidating, but this is just two screen shots from an online gene database. It's tabs S2 and S3 that scare me (or at least it would take me a while to get them into a usable form). If you scroll down on either of these tabs you'll see that the headers repeat themselves and the data is not all neatly in a line as you would find in an SPSS file. Carry on further with R and you'll learn how to work with this, and other even messier, kind of data, and convert them into what is known as tidy data. 6.2.3 Reading data into RStudio So long as you have set your working directory (and you have loaded the tidyverse package) the following code, adapted for your file name, will read the data into RStudio and assign it to an object. raw_data &lt;- read_csv(&quot;Bravo-Gutierrez figure 2 data.csv&quot;) Why use read_csv rather than read.csv If you're typing the code and paying attention to the autocompleate you may have noticed read.csv as an additional option. The read.csv function comes from base R, while read_csv is from the readr package, part of the tidyverse. While it requires an extra step to install and load read_csv its worth it for three reasons: Unlike read.csv, read_csv does not convert character strings into factors by default. This gives you more control over how your data is read and interpreted. read_csv returns tibbles rather than traditional data frames. While they may seem identical, tibbles have several advantages that make them work better with the rest of the tidyverse packages. For instance, tibbles retain the data type of each column, do not convert character vectors to factors by default, and have improved printing methods. read_csv handles missing values (NAs) in a more straightforward manner and makes data cleaning and manipulation processes easier. Is this the dullest note in the whole book? Maybe, I at least feel it's a strong contender. 6.2.4 Test yourself What does the working directory do? It is the location on your computer where RStudio runs and accesses your filesIt gives you extra RAM to analyse your data more quicklyIt changes the colour theme of RStudio What is a CSV file? A file specially made for use in RA general use text file storing tabular data, separated by commasA file for storing supplementary figures for a paper If you were to run the above code, what would you then type to view this data file in a window? Can you read an SPSS file into R? That program is dead to us now. Never speak those letters again!Yes, check out the Haven package for more on this topic 6.3 Data wrangling Data wrangling refers to the process of cleaning, structuring, and enriching raw data into a more suitable format for analysis. The dplyr package (again contained in the tidyverse) is particularly useful for this as it contains a range of six functions known as the Wickham Six. The Wickham Six: filter() - A function used to select rows in a dataset based on their values. select() - A function used to choose specific columns in a dataset, potentially dropping others. mutate() - A function used to create new columns in your dataset that are functions of existing columns (i.e. computing a latent variable out of individual questionnaire items). arrange() - A function used to reorder rows in a dataset based on the values in one or several columns. summarise() - A function used to generate summary statistics of different columns in your dataset. group_by() - A function, often used with summarise(), to group the data by certain criteria before generating summary statistics. These functions are powerful and provide the building blocks for data wrangling in R. By chaining these functions together using the pipes (%&gt;%), we can perform complex data manipulations in a clean, readable, and efficient manner. This week we'll be using select(), mutate(), group_by() and summarise(), and we'll be using them a lot throughout this module so you may want to note them down on a sticky note and attach them to your screen. That's what I did when I was first learning. 6.3.1 \"Selecting\" variables for our analysis Looking back at the figure and its description in the paper we only actually need three variables from our dataset to create the boxplots. So, the first thing we'll do is tidy up the dataset by pulling out those variables and putting them in a new object called fig2_data. The code below uses the select() function, takes the variables Sample_ID, %5mc-AHRR &amp; %5mc-PRSS23 from our raw data and assigns them a new object called fig2_data. Variable names can be a bit fiddly at times. Typically they should start with a letter and be free of spaces. Often, when importing from other sources, this rule is not adhered to. To cope with this R assigns such variable flanked by back-ticks as can be seen with our %5mc-AHRR and %5mc-PRSS23 in the code below. fig2_data &lt;- raw_data %&gt;% select(Sample_ID, `%5mc-AHRR`,`%5mc-PRSS23`) Now view() the data to check that you have each of the variables mentioned. You could also check by using the str() function Note on e numbers You may have noticed the [number]e+/-n notation in the data. In this context, the e notation, also known as scientific notation, is used to represent numbers that are too large or too small to be conveniently written in decimal form. If any data in an R variable is exceptionally large or small (in our case, small), the entire variable is displayed in this scientific notation. While it may initially seem confusing, it's quite simple - for instance, 1.000000e+2 just means the number 100. Consider it a fun little quirk of R that you'll get accustomed to over time. Have a play around with the code above and see if you can select out other variables and assign them to a different data object. This skill will be super helpful for your assignment, as it will allow you to just work on your variables of interest, instead of hacking your way through a 2000 variable file each time you want to run an analysis. 6.3.2 \"Mutating\" the Sample ID variable If you look at the Sample_ID variable in our dataset, you'll see that this variable is the variable that indicates if the participant is a non-smoker (NS) or smoker (S), however, it also lumps that information next to the participant ID number, not ideal. In the bad-old-days I would have likely gone through each of these by hand, converting the ID variable into a grouping variable. This wouldn't have been too much of an issue for this particular dataset, but just imagine doing this for a dataset with thousands of participants! Luckily, we can do this conversion with a line of code using the mutate() function. There are a number of ways to do this, but here is the way I did it for this example: fig2_data &lt;- fig2_data %&gt;% mutate(quasi_condition = case_when(str_starts(Sample_ID, &quot;NS&quot;) ~ &quot;Non-Smoker&quot;, str_starts(Sample_ID, &quot;S&quot;) ~ &quot;Smoker&quot;)) The first line tells R to just overwrite the same data object with whatever we do next. I then use the mutate function to create a new variable. This variable will be called quasi_condition (as our independent variable here is quasi-experimental) and I've told R to look at the start of each string of data in the variable Sample_ID and if the letters \"NS\" are first then write \"Non-Smoker\" to the new variable, and if the letter \"S\" is first then write \"Smoker\" to the new variable. Overwriting data This: fig2_data &lt;- fig2_data was risky. It's the kind of thing that can really mess up your code, especially if you overwrite your raw data. I've seen disasters in SPSS where students kept only a single save file and reverse-coded a variable, only to return to that file at a later date. Forgetting that they had already reverse-coded, they would inadvertently reverse it again. Although there is less chance of that here, thanks to our coding script, we should still be careful in more complicated projects. Data manipulation can become confusing and can potentially lead to unintended consequences. Let's clean up the variable names, using the rename function while we're at it. fig2_data &lt;- fig2_data %&gt;% rename(AHRR = `%5mc-AHRR`, PRSS23 = `%5mc-PRSS23`) The further you progress in R the more you'll want/need your code to be neat and tidy. Here's the code that does all of the above in a single expression thanks to more piping (%&gt;%): fig2_data &lt;- raw_data %&gt;% select(Sample_ID, `%5mc-AHRR`,`%5mc-PRSS23`) %&gt;% mutate(quasi_condition = case_when(str_starts(Sample_ID, &quot;NS&quot;) ~ &quot;Non-Smoker&quot;, str_starts(Sample_ID, &quot;S&quot;) ~ &quot;Smoker&quot;)) %&gt;% rename(AHRR = `%5mc-AHRR`, PRSS23 = `%5mc-PRSS23`) If you were to view(fig2_data) now you will see a new variable has been created with our smoker/non-smoker information and the variable names are easier to handle. 6.3.3 \"Grouping\" and \"Summarising\" the data The code below takes the fig2_data object, groups it by our new condition variable, then uses the summarise function (not to be confused with the summary function) to give us the number of participants and the mean, median, and standard deviation of just the AHRR variable split by smokers and non-smokers. There is also some missing data so I've added na.rm = TRUE to each of the arguments. We'll come back to how to deal with missing data again in later weeks. fig2_data %&gt;% group_by(quasi_condition) %&gt;% summarise(N = n(), Mean = mean(AHRR, na.rm = TRUE), Median = median(AHRR, na.rm = TRUE), SD = sd(AHRR, na.rm = TRUE)) If you take a look at the output in your console, you can see how it compares to the figure from the original paper (see top of page). Boxplots have a thick black line for median of the data so for smokers a medium of 47 and for non-smokers a medium of 100 looks good to me. 6.3.4 Recreating the figure Next we can pipe our data into ggplot to create a basic boxplot of the AHRR methylation variable as separated by our quasi_condition independent variable. fig2_data %&gt;% ggplot(aes(x=quasi_condition, y=AHRR)) + geom_boxplot() And then to turn it into the final figure all we need to do is the following: my_comparisons &lt;- list(c(&quot;Smoker&quot;, &quot;Non-Smoker&quot;)) fig2_data %&gt;% ggplot(aes(x=quasi_condition, y=AHRR, fill=quasi_condition)) + geom_boxplot() + scale_fill_brewer(palette=&quot;Dark2&quot;) + stat_compare_means(comparisons = my_comparisons) + geom_jitter(width = .25) + ylab(label = &quot;%5mc-cg05575921&quot;) + theme(legend.position = &quot;none&quot;) What coding can sometimes feel like Sorry, that one probably felt a little like this: However, this is why it's amazing we now have ChatGPT. Click this link to see how great the AI is at explaining code. 6.4 Video walkthrough 6.5 Test yourself exercise Create a new script and see if you can follow the wrangling steps again, this time to create: A dataframe containing the summary statistics for the PRSS23 gene A boxplot as close to Figure 2b from the original paper as you can. Note: to match it exactly you'll need to add this additional line of code that will re-scale the y-axis: scale_y_continuous(limits=c(0, 10), name = \"PRSS23\") Solution fig2b_discriptives &lt;- fig2_data %&gt;% group_by(quasi_condition) %&gt;% summarise(N = n(), Mean = mean(PRSS23, na.rm = TRUE), Median = median(PRSS23, na.rm = TRUE), SD = sd(PRSS23, na.rm = TRUE)) my_comparisons &lt;- list(c(&quot;Smoker&quot;, &quot;Non-Smoker&quot;)) fig2_data %&gt;% ggplot(aes(x=quasi_condition, y=PRSS23, fill=quasi_condition)) + geom_boxplot() + scale_fill_brewer(palette=&quot;Dark2&quot;) + stat_compare_means(comparisons = my_comparisons) + geom_jitter(width = .25) + theme(legend.position = &quot;none&quot;) Or: fig2_data %&gt;% ggplot(aes(x=quasi_condition, y=PRSS23, fill=quasi_condition)) + geom_boxplot() + scale_fill_brewer(palette=&quot;Dark2&quot;) + stat_compare_means(comparisons = my_comparisons) + scale_y_continuous(limits=c(0, 10), name = &quot;PRSS23&quot;) + geom_jitter(width = .25) + theme(legend.position = &quot;none&quot;) Yes, I too am annoyed about the missing p-value when you adjust the y-axis. I think they calculated their p-values separately and then added them as a text geom. That would explain the slightly different p-value roundings. Next tidy up the figure by 1. Adding a title. 2. Removing the label quasi_conditions. 3. Changing the colour scheme. 4. Changing the amount of jitter. Hint Check back at the code from last week. Solution fig2_data %&gt;% ggplot(aes(x=quasi_condition, y=PRSS23, fill=quasi_condition)) + geom_boxplot() + scale_fill_viridis_d() + stat_compare_means(comparisons = my_comparisons) + geom_jitter(width = .1) + ggtitle(label = &quot;Figure 2. Methylation percentage between smokers and non-smokers.&quot;) + ylab(label = &quot;PRSS23&quot;) + xlab(label = &quot;&quot;)+ theme(legend.position = &quot;none&quot;) This book is a work in progress. If you have any feedback or spot any mistakes please enter them into this feedback survey quoting the week your comment is referring to. Thank you! :-) "],["week-4---working-with-catagorical-variables.html", "Chapter 7 Week 4 - Working with catagorical variables 7.1 Missing data 7.2 Preparing data for analysis 7.3 Visual representation using catagorical data 7.4 Discriptive statistics tables for catagorical data 7.5 Video walkthrough 7.6 Test yourself exercises:", " Chapter 7 Week 4 - Working with catagorical variables This week, we'll take our first steps towards working on your assignment dataset by looking at a similar dataset from 2019. Download the files Week4_data_shes19.csv and Codebook.csv from this week's Moodle area. Save these in the same folder as a fresh R analysis script, then set this folder as your working directory. Then read the data and codebook into RStudio and take an initial look at both. With this dataset we will: Check for missing data. Select() and filter() the data into more manageable segments. Dichotomies continuous variables. Create summary statistics for use in a descriptive statistics table. This week you will need the following packages. Don't forget, if you've not used a package previously you'll need to run install.package(\"packagename\") for each new package. library(tidyverse) library(scales) # useful for scaling and formatting data Reminder: Setting your working directory and reading data To set your working directory click Session -&gt; Set Working Directory -&gt; Choose Directory (and navigate to your folder) or To Source File Location. To load the data, first make sure you have loaded the tidyverse package. Then, read in the data: raw_data &lt;- read_csv(&quot;Week4_data_shes19.csv&quot;) Codebook &lt;- read_csv(&quot;Codebook.csv&quot;) Hopefully, you now see a dataset in your environment window comprising of 33 variables and an impressive 6881 observations (participants) - manual recoding is certainly not going to be feasible with this volume of data! This dataset is a small selection of key variables from the Scottish Health Survey for 2019. Accompanying the main dataset is an auxiliary file - the codebook. If you execute view(Codebook) or Codebook %&gt;% print(n=33), you'll find that this is a dataframe composed of two variables that provide insight into the meanings of each variable name. In some cases, they even clarify the exact question that was asked of the participants. What does the variable Cigwend represent? Time spent smoking cigarettes on weekend daysWeekend goals to quit cigarettesNumber cigarettes smoked on weekend days What is the exact variable name for the Index of Multiple Deprivation statistic used in this survey 7.1 Missing data Massive surveys are prone to missing data issues. For the majority of cases in this 2019 datasets (and your 2021 assignment dataset) the missing data will be due to different waves of a survey asking different questions or collecting different data. In this dataset, there are seven different ways in which missing data is labelled. The code below cleans this up and just labels all of these types of missing data as NA, which stands for Not Applicable. Add it to your script and run it now. missing_values &lt;- c(&quot;Unclassifiable&quot;, &quot;Refused&quot;, &quot;Don&#39;t know&quot;, &quot;Schedule not obtained&quot;, &quot;Schedule not applicable&quot;, &quot;Not applicable&quot;, &quot;Reading not obtained&quot;) cleaned_data &lt;- raw_data %&gt;% mutate_all(~replace(., . %in% missing_values, NA)) Click for an explanation of this code Simple explanation: The code generates an object named missing_values containing the different words used to identify missing data in the dataset. Then, it applies the mutate_all function to the raw data. This function replaces all occurrences of the values listed in the object missing_values to the entire dataframe with NA. It then assigns it to the object cleaned_data. Deeper explanation: The ~ symbol in the ~replace(., . %in% missing_values, NA) piece of the R code is used to create a lambda function, an on-the-fly function meant for one-time use, where . acts as a placeholder for the data being processed. This lambda function is then applied to every column in the dataframe by mutate_all(), replacing any element found in missing_values with NA. Here's a simplified example: df &lt;- data.frame(a = 1:3, b = 4:6) df_squared &lt;- df %&gt;% mutate_all(~ .^2) In this example, ~ .^2 is the lambda function. The ~ symbol tells R that what follows is a formula, and the . is a placeholder that represents the current column of data being processed. Now, let's see what the damage is, how many missing values are we dealing with? missing_data &lt;- cleaned_data %&gt;% summarise_all(~sum(is.na(.))) %&gt;% pivot_longer(cols = 1:33, names_to = &quot;Variables&quot;, values_to = &quot;NAs&quot;) %&gt;% print(n=33) Reminder: how to work out what code does Don't forget if you every come across code that you don't understand you can always use ? to find out more about each function. For example here you could run each of the following: ?summarise_all ?pivot_longer ?print Or for a plain language explanation ask your AI of choice. Here is the explanation the ChatGPT gave me for this code. Of course, Google is also an option. I'm just finding the AI is quicker these days. How many missing values are there for the variable UniCredit? What is the exact variable name for the variable with the most missing data? This second question probably lead to you looking through the data trying to work out which number is the highest. Sounds like an inefficient use of time to me. This is exactly the kind of thing we were trying to get away from with our move to R. Let's use code to make this task easier, scalable, and less prone to error. Seeing as we have this missing data in a data object, all we need to do now is arrange() the dataset in descending order and our top variable when we view the data object will be the one with the most missing data. missing_data &lt;- missing_data %&gt;% arrange(desc(NAs)) To make life even easier we could graph it: missing_data %&gt;% arrange(NAs) %&gt;% ggplot(aes(x = NAs, y = reorder(Variables, NAs))) + geom_col() 7.2 Preparing data for analysis Next, let's take a closer look at those participants that have readings for Diastolic and Systolic blood pressure. To obtain this data, an in-person visit would likely have been required, so it's no surprise that there is a fair amount of missing data. However, even with all that missing data it still looks like we have over 1000 participants with readings, which is excellent. Just think of how many hours it would take us to collect that amount of data! To prepare our data for analysis, we need to reoganise the data a little using various functions, such as filter(), mutate(), and select(). 7.2.1 Filter relevant cases The filter() function will need to be used to select only the data where there is no missing data for both the Systolic and Diastolic variables. Filtering using logical operators The filter() function is really useful but there is some extra language (logical operators) that you'll need to learn (or just learn how to find and use) to use it to its full effect. Here is the full list with their definition: &gt; (Greater Than): This operator returns TRUE if the value on its left is greater than the value on its right. For example, filter(Age &gt; 18) would keep only the rows where Age is greater than 18. &lt; (Less Than): This operator returns TRUE if the value on its left is less than the value on its right. For example, filter(Age &lt; 18) would keep only the rows where Age is less than 18. &gt;= (Greater Than or Equal To): This operator returns TRUE if the value on its left is greater than or equal to the value on its right. For example, filter(Age &gt;= 18) would keep only the rows where Age is 18 or older. &lt;= (Less Than or Equal To): This operator returns TRUE if the value on its left is less than or equal to the value on its right. For example, filter(Age &lt;= 18) would keep only the rows where Age is 18 or younger. == (Equal To): This operator returns TRUE if the value on its left is equal to the value on its right. For example, filter(Age == 18) would keep only the rows where Age is exactly 18. A common mistake here is to use just one equals sign. So common in fact that their is a tailored error message that read \"did you mean ==?\" when used incorrectly. &amp; (AND): This operator returns TRUE if both its left and right operands are TRUE. For example, filter(Age &gt; 18 &amp; Income &gt; 50000) would keep only the rows where the age is greater than 18 AND the variable Income is greater than 50,000. | (OR): This operator returns TRUE if either (or both) of its operands are TRUE. For example, filter(Age &lt; 18 | Income &gt; 50000) would keep rows where the age is less than 18 OR the income is greater than 50,000. ! (NOT): This operator negates the truth value of its operand. For example, filter(!is.na(Age)) would keep only the rows where Age is not NA. != (Not Equal To): This operator returns TRUE if the value on its left is not equal to the value on its right. For example, filter(Age != 18) would keep all the rows where Age is not 18. The following code filters only the rows where both (&amp;) Systolic and Diastolic does not include missing data (!is.na). BP_data &lt;- cleaned_data %&gt;% filter(!is.na(Systolic) &amp; !is.na(Diastolic)) 7.2.2 Additional tidying If you were to run str(cleaned_data) you will see that each variable has either a chr or num next to them. This represents the variables data type, chr stands for character data (e.g. strings of text that can be numbers or characters) and num stands for numeric data(e.g. a string of numbers). When we imported the data, our Systolic and Diastolic data variables used numbers for the BP readings but also contained two types of text response \"Not applicable\" and \"Reading not obtained\", both of which we recoded as NA. This left us with just the numerical data, however, seeing as text was present when we first read in the data for these variables were initially assigned as character (chr) data. All this means is that when we want to use this data as numeric data we need to convert them into number (num) data. The code below uses mutate() to overwrite the Systolic and Diastolic variables with the same data treated, as the function implies, as numeric type data. Finally, it might be worth using the select() function to take only the variables that we will use in our analysis. In this case we'll take the variables Age, Sex, SIMD, UrbanRural, Systolic and Diastolic. select() also lets us organise the order of the variables depending on the order we list them. BP_data &lt;- cleaned_data %&gt;% filter(!is.na(Systolic) &amp; !is.na(Diastolic)) %&gt;% mutate(Systolic = as.numeric(Systolic), Diastolic = as.numeric(Diastolic))%&gt;% select(ID, Age, Sex, SIMD, UrbanRural, Systolic, Diastolic) Note on tidy code When I've a potentially long line of code, as our select function could be, if we wanted many more variables I often use a line break to list them one after the other. One of the great features of RStudio is its ability to automatically handle text alignment. This capability not only enhances code readability but can also act as a helpful tool in error detection, as any discrepancies might result in unusual indentation. Both of the following expressions are functionally the same Tidy code that's easy to read: BP_data &lt;- cleaned_data %&gt;% filter(!is.na(Systolic) &amp; !is.na(Diastolic)) %&gt;% mutate(Systolic = as.numeric(Systolic), Diastolic = as.numeric(Diastolic))%&gt;% select(ID, Age, Sex, SIMD, UrbanRural, Systolic, Diastolic) Horrifying code, probably written by a psychopath: BP_data &lt;- cleaned_data %&gt;% filter(!is.na(Systolic) &amp; !is.na(Diastolic)) %&gt;% mutate(Systolic = as.numeric(Systolic), Diastolic = as.numeric(Diastolic))%&gt;% select(ID, Age,Sex,SIMD,UrbanRural,Systolic,Diastolic) 7.3 Visual representation using catagorical data Quite often with blood pressure data we want to use this data to work out if someone has a dangerously high blood pressure, the diagnoses of which is Hypertension. This cut off point is considered as over 140mmHg on Systolic or over 90mmHg on Diastolic measurements NHS Below is a scatter plot of diastolic against systolic blood pressure from this data. The dotted purple line is the systolic cut off for high blood pressure and the dotted red line is the diastolic blood pressure cut off for high blood pressure. As such, we want to assign the label of hypertension to all data points above the purple line and to the right of the red line. Code for plot BP_data %&gt;% ggplot(aes(x = Diastolic, y = Systolic)) + geom_point() + geom_vline(xintercept = 90, linetype = &quot;dotted&quot;, color = &quot;red&quot;, size = 1) + geom_hline(yintercept = 140, linetype = &quot;dotted&quot;, color = &quot;purple&quot;, size = 1) + labs(title = &quot;Scatter Plot of Systolic and Diastolic Blood Pressure&quot;, x = &quot;Diastolic Blood Pressure&quot;, y = &quot;Systolic Blood Pressure&quot;) The following code creates a new variable, using the mutate() function. This variable is named BP_cat and uses an if_else function that says if Systolic is above 140 or (denoted by the |) Diastolic is above 90, is TRUE, then assign \"Hypertension\", else assign \"Normal BP range\". BP_data_labelled &lt;- BP_data %&gt;% mutate(BP_cat = if_else(Systolic &gt; 140 | Diastolic &gt; 90, &quot;Hypertension&quot;, &quot;Healthy BP range&quot;)) The nice thing about this kind of problem is we can then check it on the scatter plot by assigning our new variable to the colour aesthetic. Code for plot BP_data_labelled %&gt;% ggplot(aes(x = Diastolic, y = Systolic, colour = BP_cat)) + geom_point() + labs(title = &quot;Scatter Plot of Systolic and Diastolic Blood Pressure&quot;, x = &quot;Diastolic Blood Pressure&quot;, y = &quot;Systolic Blood Pressure&quot;) + scale_color_discrete(name = &quot;BP Categories&quot;) Hopefully, by looking at the graph you can see how problematic a hard cut like this can be in health statistics. 7.4 Discriptive statistics tables for catagorical data One of the most common uses for categorical data is to segment your data for a frequency table. The following code groups the data by two variables of interest, in this case our newly created blood pressure categorical variable and our Sex variable percentage_Sex_data2 &lt;- BP_data_labelled %&gt;% group_by(Sex, BP_cat) %&gt;% summarise(Count = n()) %&gt;% mutate(Percentage = Count / sum(Count), Percentage = percent(Percentage, accuracy = 0.01)) # Note: the percent function requires the &quot;scales&quot; package We can then just adapt this code for another variable percentage_UR_data &lt;- BP_data_labelled %&gt;% group_by(UrbanRural, BP_cat) %&gt;% summarise(Count = n()) %&gt;% mutate(Percentage = Count / sum(Count), Percentage = percent(Percentage, accuracy = 0.01)) For now, it might be easier for you to view the date object, copy the data and create your own APA-formatted table in Microsoft Word. However, if you're becoming like me and are obsessed with doing absolutely everything in R, with a bit of work, you can create tables ready for publication right from RStudio. Code for making this table This process takes effort and might not be worth it for a single table. However, if you ever have to create monthly reports, this sort of automated workflow could save you significant time in the future. Also, there's likely a far easier way than I've done it here. If you find one, please do let me know! # this table was created using the &quot;gt&quot; package. Install it just as you would any other package. library(gt) # The following wrangles the data into the right formation for the table percentage_Sex_data &lt;- BP_data_labelled %&gt;% group_by(Sex, BP_cat) %&gt;% summarise(Count = n()) %&gt;% mutate(Percentage = Count / sum(Count), Percentage = percent(Percentage, accuracy = 0.01)) %&gt;% pivot_wider(names_from = &quot;BP_cat&quot;, values_from = c(Count, Percentage))%&gt;% mutate(Stat_Healthy = paste(`Count_Healthy BP range`, &quot; (&quot;, `Percentage_Healthy BP range`, &quot;)&quot;, sep=&quot;&quot;)) %&gt;% mutate(Stat_Hyper = paste(Count_Hypertension, &quot; (&quot;, Percentage_Hypertension, &quot;)&quot;, sep=&quot;&quot;))%&gt;% rename(variable = Sex) %&gt;% mutate(variable_name = &quot;Sex&quot;) percentage_UR_data &lt;- BP_data_labelled %&gt;% group_by(UrbanRural, BP_cat) %&gt;% summarise(Count = n()) %&gt;% mutate(Percentage = Count / sum(Count), Percentage = percent(Percentage, accuracy = 0.01)) %&gt;% pivot_wider(names_from = &quot;BP_cat&quot;, values_from = c(Count, Percentage)) %&gt;% mutate(Stat_Healthy = paste(`Count_Healthy BP range`, &quot; (&quot;, `Percentage_Healthy BP range`, &quot;)&quot;, sep=&quot;&quot;)) %&gt;% mutate(Stat_Hyper = paste(Count_Hypertension, &quot; (&quot;, Percentage_Hypertension, &quot;)&quot;, sep=&quot;&quot;))%&gt;% rename(variable = UrbanRural) %&gt;% mutate(variable_name = &quot;Location&quot;) table1_data &lt;- rbind(percentage_Sex_data, percentage_UR_data) %&gt;% select(variable_name, variable, Stat_Healthy, Stat_Hyper) # Note: it is really important to get the group_by vaiable in the right order. See what happens when you flip them. # the following uses the gt package to create a table in APA style. gt(table1_data, groupname_col = &quot;variable_name&quot;) %&gt;% tab_header(title = md(&quot;**Table 1**&quot;), subtitle = md(&quot;Demographic statistics by blood pressure catagories&quot;)) %&gt;% opt_align_table_header(align = &quot;left&quot;) %&gt;% cols_label(variable = &quot; &quot;, Stat_Healthy = md(&quot;**Healty BP Range &lt;br&gt; n(%)**&quot;), Stat_Hyper = md(&quot;**Hypertention &lt;br&gt; n(%)**&quot;)) %&gt;% tab_options(table.border.top.color = &quot;white&quot;, heading.title.font.size = px(16), column_labels.border.top.width = 3, column_labels.border.top.color = &quot;black&quot;, column_labels.border.bottom.width = 3, column_labels.border.bottom.color = &quot;black&quot;, table_body.border.bottom.color = &quot;black&quot;, table.border.bottom.color = &quot;white&quot;, table.width = pct(50), table.background.color = &quot;white&quot;) %&gt;% cols_align(align=&quot;center&quot;) %&gt;% tab_style(style = list(cell_borders(sides = c(&quot;top&quot;, &quot;bottom&quot;), color = &quot;white&quot;, weight = px(1)), cell_text(align=&quot;center&quot;), cell_fill(color = &quot;white&quot;, alpha = NULL)), locations = cells_body(columns = everything(), rows = everything())) 7.5 Video walkthrough 7.6 Test yourself exercises: 7.6.1 Exercise 1 Dichotomise the age variable using a median split. Hint Take the medium of the variable and assign those below or equal to the median as \"Young\" and those greater than the medium as \"Old\". There are likely a number of ways to find the medium of the data. I find the summary function to be the quickest. You'll then need to use if_else within a mutate function to create a new variable with the young and old labels. Look back at the code from earlier and adapt it accordingly. Solution BP_data_labelled %&gt;% select(Age) %&gt;% summary() age_data_labelled &lt;- BP_data_labelled %&gt;% mutate(age_cat = if_else(Age &lt;= 54, &quot;Young&quot;, &quot;Old&quot;)) 7.6.2 Exercise 2 Create a plot of Diastolic compared to Systolic with colour indicating either a \"Old\" or \"Young\" participant. Hint Look back at the code for the plot that I created for Hypertension and Health BP Range. Substitute your new variable (I named mine age_cat) for the variable I used to dichotomise BP. Solution age_data_labelled %&gt;% ggplot(aes(x = Diastolic, y = Systolic, colour = age_cat)) + geom_point() + labs(title = &quot;Scatter Plot of Systolic and Diastolic Blood Pressure&quot;, x = &quot;Diastolic Blood Pressure&quot;, y = &quot;Systolic Blood Pressure&quot;) 7.6.3 Exercise 3 Create the descriptive statistics for hypertension by your new dicotomised age variable. Solution percentage_age_data &lt;- age_data_labelled %&gt;% group_by(age_cat, BP_cat) %&gt;% summarise(Count = n()) %&gt;% mutate(Percentage = Count / sum(Count), Percentage = percent(Percentage, accuracy = 0.01)) 7.6.4 Exercise 4 Create multiple age ranges i.e. &lt;18, 18 - 30, 31 - 40, 41 - 50, 51 - 60, 60+ and rerun the descriptive statistics. For this you'll need to investigate how to use the case_when function. Solution BP_data_labelled2 &lt;- BP_data_labelled %&gt;% mutate(age_cat = case_when( Age &lt; 18 ~ &quot;&lt;18&quot;, Age &gt;= 18 &amp; Age &lt;= 30 ~ &quot;18-30&quot;, Age &gt; 30 &amp; Age &lt;= 40 ~ &quot;31-40&quot;, Age &gt; 40 &amp; Age &lt;= 50 ~ &quot;41-50&quot;, Age &gt; 50 &amp; Age &lt;= 60 ~ &quot;51-60&quot;, Age &gt; 60 ~ &quot;60+&quot; )) percentage_age_data &lt;- BP_data_labelled2 %&gt;% group_by(age_cat, BP_cat) %&gt;% summarise(Count = n()) %&gt;% mutate(Percentage = Count / sum(Count), Percentage = percent(Percentage, accuracy = 0.01)) 7.6.5 Exercise 5 Create the count and percentages for the Scottish Index of Multiple Deprivation variable. Solution percentage_SIMD_data &lt;- BP_data_labelled2 %&gt;% group_by(SIMD, BP_cat) %&gt;% summarise(Count = n()) %&gt;% mutate(Percentage = Count / sum(Count), Percentage = percent(Percentage, accuracy = 0.01)) 7.6.6 Exercise 6 (very much optional / just for fun) Create a table that includes the frequency and percentage of health and hypertensive participants for Sex, UrbanRural, Age SIMD. And if you find a quicker and easier way to do this please do let me know! Solution # copied from earlier table code percentage_Sex_data &lt;- BP_data_labelled2 %&gt;% group_by(Sex, BP_cat) %&gt;% summarise(Count = n()) %&gt;% mutate(Percentage = Count / sum(Count), Percentage = percent(Percentage, accuracy = 0.01)) %&gt;% pivot_wider(names_from = &quot;BP_cat&quot;, values_from = c(Count, Percentage))%&gt;% mutate(Stat_Healthy = paste(`Count_Healthy BP range`, &quot; (&quot;, `Percentage_Healthy BP range`, &quot;)&quot;, sep=&quot;&quot;)) %&gt;% mutate(Stat_Hyper = paste(Count_Hypertension, &quot; (&quot;, Percentage_Hypertension, &quot;)&quot;, sep=&quot;&quot;))%&gt;% rename(variable = Sex) %&gt;% mutate(variable_name = &quot;Sex&quot;) # copied from earlier table code percentage_UR_data &lt;- BP_data_labelled2 %&gt;% group_by(UrbanRural, BP_cat) %&gt;% summarise(Count = n()) %&gt;% mutate(Percentage = Count / sum(Count), Percentage = percent(Percentage, accuracy = 0.01)) %&gt;% pivot_wider(names_from = &quot;BP_cat&quot;, values_from = c(Count, Percentage)) %&gt;% mutate(Stat_Healthy = paste(`Count_Healthy BP range`, &quot; (&quot;, `Percentage_Healthy BP range`, &quot;)&quot;, sep=&quot;&quot;)) %&gt;% mutate(Stat_Hyper = paste(Count_Hypertension, &quot; (&quot;, Percentage_Hypertension, &quot;)&quot;, sep=&quot;&quot;))%&gt;% rename(variable = UrbanRural) %&gt;% mutate(variable_name = &quot;Location&quot;) # code adapted for age percentage_age_data &lt;- BP_data_labelled2 %&gt;% group_by(age_cat, BP_cat) %&gt;% summarise(Count = n()) %&gt;% mutate(Percentage = Count / sum(Count), Percentage = percent(Percentage, accuracy = 0.01)) %&gt;% pivot_wider(names_from = &quot;BP_cat&quot;, values_from = c(Count, Percentage)) %&gt;% mutate(Stat_Healthy = paste(`Count_Healthy BP range`, &quot; (&quot;, `Percentage_Healthy BP range`, &quot;)&quot;, sep=&quot;&quot;)) %&gt;% mutate(Stat_Hyper = paste(Count_Hypertension, &quot; (&quot;, Percentage_Hypertension, &quot;)&quot;, sep=&quot;&quot;))%&gt;% rename(variable = age_cat) %&gt;% mutate(variable_name = &quot;Age&quot;) # code adapted for SIMD percentage_SIMD_data &lt;- BP_data_labelled2 %&gt;% group_by(SIMD, BP_cat) %&gt;% summarise(Count = n()) %&gt;% mutate(Percentage = Count / sum(Count), Percentage = percent(Percentage, accuracy = 0.01)) %&gt;% pivot_wider(names_from = &quot;BP_cat&quot;, values_from = c(Count, Percentage)) %&gt;% mutate(Stat_Healthy = paste(`Count_Healthy BP range`, &quot; (&quot;, `Percentage_Healthy BP range`, &quot;)&quot;, sep=&quot;&quot;)) %&gt;% mutate(Stat_Hyper = paste(Count_Hypertension, &quot; (&quot;, Percentage_Hypertension, &quot;)&quot;, sep=&quot;&quot;))%&gt;% rename(variable = SIMD) %&gt;% mutate(variable_name = &quot;Scottish Index of Multiple Deprivation&quot;) table2_data &lt;- rbind(percentage_Sex_data, percentage_UR_data, percentage_age_data, percentage_SIMD_data) %&gt;% select(variable_name, variable, Stat_Healthy, Stat_Hyper) If you look at the table2_data data frame you'll see that the SIMD variables are out of order. That's annoying. The order we want is: \"Most deprived\" (the areas with the highest levels of deprivation). \"2nd\" second most deprived. \"3rd\" third most deprived. \"4th\" fourth most deprived. \"Least deprived\" (the areas with the lowest levels of deprivation). The following code, creates a new data object with the order I want the variables and then joins it to the SIMD object that contains the percentages, and then arranges it by the custom order. Typically, I would just rework my earlier code if I run into this kind of problem. order_df &lt;- data.frame(variable = c(&quot;Most deprived&quot;, &quot;2nd&quot;, &quot;3rd&quot;, &quot;4th&quot;, &quot;Least deprived&quot;), order = c(1, 2, 3, 4, 5)) percentage_SIMD_data &lt;- percentage_SIMD_data %&gt;% left_join(order_df, by = &quot;variable&quot;) %&gt;% arrange(order) table2_data &lt;- rbind(percentage_Sex_data, percentage_UR_data, percentage_age_data, percentage_SIMD_data) %&gt;% select(variable_name, variable, Stat_Healthy, Stat_Hyper) The following code creates the APA style table using the gt package.The only thing I did here was to change the table1_data to table2_data and change the title to \"Table 2\". You may need to click Zoom to view the table in its full glory. gt(table2_data, groupname_col = &quot;variable_name&quot;) %&gt;% tab_header(title = md(&quot;**Table 2**&quot;), subtitle = md(&quot;Demographic statistics by blood pressure catagories&quot;)) %&gt;% opt_align_table_header(align = &quot;left&quot;) %&gt;% cols_label(variable = &quot; &quot;, Stat_Healthy = md(&quot;**Healty BP Range &lt;br&gt; n(%)**&quot;), Stat_Hyper = md(&quot;**Hypertention &lt;br&gt; n(%)**&quot;)) %&gt;% tab_options(table.border.top.color = &quot;white&quot;, heading.title.font.size = px(16), column_labels.border.top.width = 3, column_labels.border.top.color = &quot;black&quot;, column_labels.border.bottom.width = 3, column_labels.border.bottom.color = &quot;black&quot;, table_body.border.bottom.color = &quot;black&quot;, table.border.bottom.color = &quot;white&quot;, table.width = pct(50), table.background.color = &quot;white&quot;) %&gt;% cols_align(align=&quot;center&quot;) %&gt;% tab_style(style = list(cell_borders(sides = c(&quot;top&quot;, &quot;bottom&quot;), color = &quot;white&quot;, weight = px(1)), cell_text(align=&quot;center&quot;), cell_fill(color = &quot;white&quot;, alpha = NULL)), locations = cells_body(columns = everything(), rows = everything())) This book is a work in progress. If you have any feedback or spot any mistakes please enter them into this feedback survey quoting the week your comment is referring to. Thank you! :-) "],["week-5---simple-linear-regression.html", "Chapter 8 Week 5 - Simple Linear Regression 8.1 Effects of noise sensitivity and road traffic noise on the immune system. 8.2 Correlation matrix 8.3 Simple Regression Plots. 8.4 Video walkthrough 8.5 Test yourself exercise", " Chapter 8 Week 5 - Simple Linear Regression This week we'll be looking at the data behind a paper titled Effects of self-reported sensitivity and road-traffic noise levels on the immune system by Kim et al (2017). If I'm being honest, the findings of the paper are slightly underwhelming, but none-the-less their shared data allows us to apply some of our existing R skills and learn some new functions that will be directly applicable to your assignment. The paper is open access and available in full here: Kim et al (2017). Familiarise yourself with the details of the study and then we'll take a look at replicating some of the basic correlational findings. The authors shared their data, but did the authors share their data in an easy to use format? No of course not, they shared it as a PDF. Luckily for you, I have converted their PDF data file into a CSV file that you can find on this weeks Moodle page. How I converted this data from a PDF to CSV Thankfully the data was fairly tidy on the PDF, so it didn't take too much effort to convert it, but it did involve me having to mess around on excel, which is never fun. If you would like to repeat the process yourself, here are the steps: In Excel go to the Data tab Get Data -&gt; From File -&gt; From PDF. Select the downloaded PDF file and click Import. Then in the navigation window select Select multiple items, then tick either the three pages or the three tables. Then click on the drop arrow next to Load and click Load to. On the next window select Table then ok. This will give you three tabs of data. Then it's a matter of combining these data. I did this manually through copy and pasting but no doubt there's a smarter way of doing this. I'm explaining this here partly so that you know how to do this if you every come across similar data in the future, but mainly this is just so I have a note of this for when I inevitably forget how to do this next time. In fact, most of this book is just notes to help \"future me\" remember how to code. 8.1 Effects of noise sensitivity and road traffic noise on the immune system. The research conducted by Kim and colleagues in 2017 set out to explore the effects of road traffic noise exposure and self-reported noise sensitivity on human health. This study falls under the domain of psychoneuroimmunology - a scientific field that investigates the interplay between psychological processes, the nervous system, and the immune system. The researchers specifically looked at levels of biomarkers related to stress and immune system activity as a potential mechanism of this impact. In the context of this study, several key health markers were investigated: Cortisol levels: Cortisol is a hormone that the body produces when under stress (see last weeks lecture for more on this topic). Natural Killer (NK) and Natural Killer T (NKT) cell populations: These cells are key components of the immune system. NK cells play a significant role in the immune response to cells that are cancerous or infected by viruses. NKT cells, a subset of T cells with properties similar to NK cells, are involved in modulating immune responses. NK cell activity: This was assessed by measuring levels of interleukin-12 (IL-12) and interferon-gamma (INF-γ), both of which are cytokines. IL-12 is a signaling molecule in the immune system that gets produced when the body encounters an intracellular pathogen. It enhances the ability of NK cells and cytotoxic T cells to kill these pathogens. INF-γ is another cytokine that has a crucial role in immune responses, particularly against viral infections and cancer. The researcher obtained data on these markers through immunological assays performed on the extracted blood sample for each participant. Each participant also gave the following data through a self-report survey: Demographic factors: Age, Education level, and income Lifestyle factors: Smoking status, Alcohol status, and Exercise status Sensitivity to Noise: as assessed on a 11-point visual analogue scale. Residential noise levels (Noise_Ldn) were calculated by the participant locating their address on a pre-existing noise map of their local area and taking the corresponding noise level in decibels. The study sets out to test for relationships between noise level and the stress and immunity biomarkers while controlling for the demographic, lifestyle and sensitivity to noise variables. Today we'll just look at the simple relationships but we'll come back to this dataset again when we look at multiple regression. As always you'll need to download the data, set your working directory, load this weeks packages, and read in the data. We'll be using a new package this week (sjPlot), so if this is the first time you are using this package you'll need to install it with the install.packages(\"sjPlot\") command. library(tidyverse) library(sjPlot) raw_data &lt;- read_csv(&quot;Kim et al data.csv&quot;) # note you likely have the data saved under a different name to the name I have used here Take a look at the dataset either visually using the view() function and/or through using the summary() or str() functions. There is a single value used throughout the dataset to indicate missing data. We need to change that value into an NA. Below is the code we used last week, replace the ? with the appropriate value and and run the code to recode missing data as NA missing_values &lt;- ? cleaned_data &lt;- raw_data %&gt;% mutate_all(~replace(., . %in% missing_values, NA)) Answer The authors use the number 999 to indicate missing data. Other than the missing data this dataset looks clean to me. 8.1.1 Replicating Kim et al 2017 correlational analyses For now, we'll only look to replicate the findings in Figure 2 (see top of page), Table 2, and the in-text reporting (see below). As you may recall from your undergraduate research methods classes, in frequentist statistical analysis, we often aim to use parametric tests when the statistical assumptions underlying these tests are met, as these tests can be more sensitive to effects if their assumptions hold. However, if these assumptions are not met, non-parametric alternatives may be more appropriate. Note: we will recap the topic of parametric assumptions in full in NS5171 When it comes to correlation analyses, the parametric test we commonly use is the Pearson correlation, and a commonly used non-parametric alternative is the Spearman's Rank correlation This study appears to have performed their analysis using a pearson correlation (we'll come back to the multiple regression in a couple of weeks time) and to satisfy the normality assumption they used a logarithmic transformation. What is Logarithmic transformation? One common method for dealing with non-normal data is to apply a transformation to the data, such as a logarithmic (log) transformation. This can make the data more normally distributed, or at least more symmetric, which can help meet statistical assumptions. A common variable that often gets a log transformation is income due to it's highly skewed nature. Most people earn a fairly similar moderate income while a few people earn far greater amounts than the bulk of individuals. Graphing this data (see below) you'll see that such a distribution skews to the right. Another way to present the same data is to make the scale logarithmic. The first figure below shows the same data as the second just with a logarithmic scale, where the difference between each point in the scale increases in an exponential manner. This change makes the data appear more normally distributed. We can achieve a similar outcome by performing a logarithmic transformation to the data. Below I have graphed the log of income for the same data. Notice how the histogram now corresponds to the above figure with the log scale. By doing such a transformation the variable not only looks normal but is statistically more normally distributed. Even though this is what the authors choose to do, its still worth us having a visually inception of the raw data and then seeing how it looks after the transformation. To do this we can run a histogram for each of the variables of interest. In our case all of the biomarkers (which they transformed) and noise and noise sensitivity variables (which they did not transform). You should already know (or be able to work out) how to run a simple histogram by now, so try it for yourself before using my code. Simple histogram code cleaned_data %&gt;% ggplot(aes(x = Cortisol)) + geom_histogram() # or this if you want to tidy it up: cleaned_data %&gt;% ggplot(aes(x = Cortisol)) + geom_histogram(fill = &quot;darkgreen&quot;, color = &quot;black&quot;) + ggtitle(&quot;Histogram of Cortisol Levels&quot;) + xlab(&quot;Cortisol Levels&quot;) + ylab(&quot;Frequency&quot;) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) We could just run our histogram code 7 times, changing out the variable name each time. However, that can often look messy in our script, so as a way to tidy things up we may want to run them all together. To do this, first we select() out these key variables. Then we can use the pivot_longer function, followed by including a facet_wrap line within our ggplot code to create a single image with all of our plots. histogram_data &lt;- cleaned_data %&gt;% select(Cortisol, NK_cells, NKT_cells, IL12, INFgamma, Noise_Ldn, Noise_sensitivity) %&gt;% pivot_longer(cols = 1:7, names_to = &quot;variables&quot;, values_to = &quot;score&quot;) What is Pivoting? Pivoting data refers to reshaping your dataset, where you transition from a 'wide' format to a 'long' format, or vice versa. In a 'wide' format, each row corresponds to a single observation, and each variable has its own column. As such, our current dataset can be said to be in a 'wide' format. In a 'long' format, each row still represents a single observation, but the variables you're examining are compiled into one column. Additional columns provide distinct information for each observation. For example, you might have one row for each measurement taken, with one column specifying the type of measurement and another giving the measurement value itself. This makes more sense when you look at the data. Here's an example of: A dataset in wide format: And here is an example of the same dataset in long format (after pivoting) If you take a look at the data after this pivoting you should see that all the scores are in one variable named score. Running our histogram code with a facet_wrap gives you all the histograms at once. histogram_data %&gt;% group_by(variables)%&gt;% ggplot(aes(x = score)) + geom_histogram(fill = &quot;darkgreen&quot;, color = &quot;black&quot;) + facet_wrap(~variables, scale = &quot;free&quot;) What do you think? Which of these variables are skewed? Answer I'm going to go with two being highly skewed (INFgammer &amp; NKT_cells) and one being close to skewed (IL12). However, without checking statistically (which I'll show you how to check later in the course) its hard to know for sure. Anyway, we'll follow what the authors did and perform a log transformation on each of the biomarkers. Now we can do the log transformation on the biomarkers and run the histograms again. cleaned_data_log &lt;- cleaned_data %&gt;% mutate(Cortisol_log = log(Cortisol), NK_cells_log = log(NK_cells), NKT_cells_log = log(NKT_cells), IL12_log = log(IL12), INFgamma_log = log(INFgamma)) Adapt the piviot and facet_wrap code from above and rerun the histograms with this log transformed data. You should end up with a range of histograms that look something like this: 8.1.2 One more annoying thing before we can actually analyse the data One of our variables is going to give us some issues in the next section due to our log transformation. If you run a summary of the dataset, or just take a look at our new data object containing the log variables, you'll see that a few data points in the NKT_cell_log variable are listed as -Inf. This is due to us trying to take the log of 0, which equals infinity. I'm not sure how Kim et al addressed this issue, however, we're going to address this by converting the few -Inf values to 0's. I've done this by adding an extra mutate function to our log transformation expression. In this line I use ifelse to say if the value in NKT_cells_log is infinite then replace with a 0, else replace with the same value again. cleaned_data_log &lt;- cleaned_data %&gt;% mutate(Cortisol_log = log(Cortisol), NK_cells_log = log(NK_cells), NKT_cells_log = log(NKT_cells), IL12_log = log(IL12), INFgamma_log = log(INFgamma)) %&gt;% mutate(NKT_cells_log = ifelse(is.infinite(NKT_cells_log), 0, NKT_cells_log)) # take a look just to check cleaned_data_log %&gt;% ggplot(aes(x = NKT_cells_log)) + geom_histogram() 8.2 Correlation matrix After all that data wrangling we are finally at the stage where we can start replicating the authors primary analysis. A single correlation can be run through the following code: cor.test(x = cleaned_data_log$Noise_Ldn, y = cleaned_data_log$IL12_log) Wait, what is a $ doing there?! This is an example of some \"base R\" coding (i.e. the R functionality that does not involve the tidyverse package). Frustratingly, the cor.test function is not compatible with our pipes (%&gt;%) from tidyverse (or even the base R pipes |&gt;) so we've had to do this the old fashioned way. All cleaned_data_log$Noise_Ldn means is: \"Take the Noise_Ldn variable from et cleaned_data_log data object\". It's worth knowing about this type of notation, as at some point you'll likely run across blogs and tutorial videos that use such code. After running that line of code you should see the following appear in your console: # Pearson&#39;s product-moment correlation # data: cleaned_data_log$Noise_Ldn and cleaned_data_log$IL12_log # t = 3.1267, df = 170, p-value = 0.00208 # alternative hypothesis: true correlation is not equal to 0 # 95 percent confidence interval: # 0.08658474 0.36992397 # sample estimates: # cor # 0.2331978 The important information for us to extract from the output is the p-value and the cor value, which indicates r, our correlation coefficient for the pearsons correlation. In this case we have a r value of 0.233 and a p-value of 0.002. In APA style we would report this finding as the following: There was a significant positive correlation between Noise level and IL-12 levels, r(170) = .233, p &lt; .01 Therefore, have we sucessfully replicated this finding? YesNo We could run all the correlations present in Table 2 separately, and then build our own table. This is a valid method for such analysis. However, considering we have 7 variables, the combinations quickly increase. Doing so would require 21 distinct lines of code and the subsequent task of extracting all the outcomes. To streamline this process, we can use the sjPlot package to generate a correlation matrix. The first expresion in the code below (after loading in the package) creates a new data object containing only the relevant data for the matrix. The second experssion assembles a correlation table using our chosen data. I've incorporated additional arguments in the code for further functionality. To view a comprehensive list of available arguments for this table, run ?tab_corr. Note: The resulting table, generated in HTML format, will display in your viewer tab rather than the plot tab. The final expression visualises the data in a colour-coded grid. I find this format more intuitive for identifying high correlations in large datasets. Similarly, running ?sjp.corr gives a full list of the available arguments for this type of plot. library(sjPlot) corr_matrix_data &lt;- cleaned_data_log %&gt;% select(Noise_Ldn, Noise_sensitivity, Cortisol_log, NK_cells_log, NKT_cells_log, IL12_log, INFgamma_log) tab_corr(data = corr_matrix_data, triangle = &quot;lower&quot;, corr.method = &quot;pearson&quot;, show.p = TRUE, title = &quot;Table 2. Correlation among noise levels, noise sensitivity and immune response.&quot;, var.labels = c(&quot;Ldn&quot;, &quot;Noise sensitivity&quot;, &quot;Cortisol&quot;, &quot;NK Cells&quot;, &quot;NKT cells&quot;, &quot;IL-12&quot;, &quot;INFgamma&quot;)) sjp.corr(data = corr_matrix_data, corr.method = &quot;pearson&quot;, show.legend = TRUE, sort.corr = FALSE, title = &quot;Figure 1. Correlation plot of variables&quot;, axis.labels = c(&quot;Ldn&quot;, &quot;Noise sensitivity&quot;, &quot;Cortisol&quot;, &quot;NK Cells&quot;, &quot;NKT cells&quot;, &quot;IL-12&quot;, &quot;INFgamma&quot;)) Were we successful in replicating the correlational findings from Kim et al (2017)? Yes, it seems we were. The primary significant findings are largely in line with the original study, though there are minor variances in the correlation coefficients. From what I can tell, these disparities appear to be influenced by the Noise Sensitivity and NKT cells variables. The variations in the NKT cells variable are understandable given that this was the variable with the -inf values. It's possible that the original authors addressed this differently than we did. As for the Noise Sensitivity variable, I'm somewhat stumped. The values are closely aligned but not identical. If you figure out why, please do get in touch and let me know what I missed here. 8.3 Simple Regression Plots. Lastly, we can now replicate the plots in Figure 2. These plots involve a simple scatter plot with the addition of a regression line, regression formula, and the R-Squared value. I will go into more detail about each of these aspects in NS7151, however, here is a quick overview of these concepts. Regression line: Adding a regression line helps capture the trend in data. The line represents a 'best fit' through the scatter points, indicating the overall relationship between the variables. Regression formula: The regression formula is the mathematical equation of the regression line, typically in the form y = mx + b. m denotes the slope (the effect of x on y), and b is the y-intercept (the y value when x is zero). R-Squared value: R-squared value, ranging from 0 to 1, shows how well the regression line fits the data. A high R-squared means a significant portion of the variation in the dependent variable can be explained by the independent variable(s). The code below runs a linear regression, modelling the relationship between Cortisol and Noise Levels. To view the model we then have to run summary() on our newly created model. The results of the model will then be outputted into the console. model_Cort_Ldn &lt;- lm(Cortisol_log ~ Noise_Ldn, data = cleaned_data_log) summary(model_Cort_Ldn) From this we can see that we have an R-Squared that matches the plot A from figure 2 and it also tells us that this relationship is not significant (p=0.399). Where is the p-value? Pr(&gt;|t|) in the output stands for \"p-value of the t-statistic\". The t-statistic measures the size of the difference relative to the variation in your sample data. A larger absolute value generally means it's more likely there is a meaningful difference. The p-value (Pr(&gt;|t|)) is the probability that you would observe such a large absolute t-value if the null hypothesis (i.e., there's no effect or relationship) were true. Smaller p-values suggest that the null hypothesis is less likely, so the effect or relationship you observed is more likely to be real and not due to random chance. In this case we're looking at the p-value of the Noise_Ldn predictor in the model. This is greater than 0.05 (the threshold we often choose in psychology for statistical significance). Therefore, this relationship is not significant (as was also found in our correlation matrix). For our figures we'll need to extract a few bits of information from our model. The first line of the code below takes just the r-squared value and stores the value in r-squared. The next line does the same with the coefficients of the model (m and b in the equation of the line). The equation expression here takes the coefficients and turns them into a string that contains the equation of our line (y=0.0025*x + 2.4). The last line does the same but with the r-squared values. r_squared &lt;- summary(model_Cort_Ldn)$r.squared coefficients &lt;- coef(model_Cort_Ldn) equation &lt;- paste0(&quot;y = &quot;, round(coefficients[2], digits = 4), &quot; * x + &quot;, round(coefficients[1], digits = 2)) r_squared &lt;- paste0(&quot;R\\u00B2 = &quot;, round(summary(model_Cort_Ldn)$r.squared, 3)) # \\u00B2 is the code for making text superscript, i.e. turning the 2 into squared. Now we can create the plot. This is very similar to other plots you have created, however, this time I've used the annotate function to add the text labels. Have a play around with the code and see if you can figure out what each of the arguments within the code do. cleaned_data_log %&gt;% ggplot(aes(x = Noise_Ldn, y = Cortisol_log)) + geom_point(shape = 1) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + scale_x_continuous(limits = c(30, 100)) + scale_y_continuous(limits = c(1.5, 3.5)) + xlab(label = &quot;Noise Level&quot;) + ylab(label = &quot;Cortisol&quot;) + ggtitle(&quot;Figure 2A: A plot to show the relationship between Noise and Cortisol levels&quot;) + annotate(&quot;text&quot;, x = 88, y = 2.8, label = equation) + annotate(&quot;text&quot;, x = 95, y = 3.5, label = r_squared) 8.4 Video walkthrough 8.5 Test yourself exercise Create the plots for each of the significant findings in Figure 2. Try and get them to resemble the original as closely as you can. A weird detail about this study Here's something wild about the study, that perhaps you noticed, Men were excluded from the analysis. The authors argued that due to Men tending \"to have more activity in environments outside of the residence target area\" and that this could \"influence [their] sensitivity to the environmental noise around the residence\". This is a bit of a red flag to me. Especially seeing as blood samples were still collected from these individuals (67 in total). And to make things worse, this: raw_data %&gt;% select(Sex)%&gt;% group_by(Sex)%&gt;% summarise(count = n()) I have to move on now, but if anyone has the time, please do rerun the analysis with these two participants filtered out. I'd be interesting to see if it makes a difference. This book is a work in progress. If you have any feedback or spot any mistakes please enter them into this feedback survey quoting the week your comment is referring to. Thank you! :-) "],["week-6---catch-up-week.html", "Chapter 9 Week 6 - Catch up week 9.1 PsyTeachR 9.2 R for data science 9.3 TidyTuesday 9.4 YouTube channels of note", " Chapter 9 Week 6 - Catch up week Well done! You've made it to the halfway point of the semester. This week we will be in the Psych Lab looking at lab based and biometric data collection methods. This seemed like a good point in the semester to give you a chance to catch up on the exercises from previous weeks and to point you towards some other resources that you may find of use: 9.1 PsyTeachR My first recommendation to you is PsyTeachR. This is the resource that the University of Glasgow use to teach their undergraduate and postgraduate psychology students R. This is the resource that I used when I was first learning to code (which was only around 2 years ago) and it's my inspiration for writing this handbook. Their Data Skills module is a good place to start. 9.2 R for data science R for data science by Wickham, Çetinkaya-Rundel &amp; Grolemund is a the way to code with the Tidyverse, as it is written by those that maintain the code. Lots of depth here, and great if you want to gain a deeper understanding rather than just rely on editing the code I give you. Also if I code one way, and they do the same task another way, do it their way! They are likely (more) right. 9.3 TidyTuesday TidyTuesday is an online community of hobbyists and professionals that work with data. Each week they release a new dataset and those in the community anlyse and visualise the data and share online with the rest of the community. Recent datasets include: UFO Sightings London Marathon data Hollywood relationship age gaps Scroll down on each of the links to give you a description of the dataset and info on how to read the dataset into R. Search #TidyTuesday on twitter and elsewhere to get an idea of what the community creates. Often they also share their code. 9.4 YouTube channels of note YouTube was indispensable when I was first learning R. This was the first video I saw on the topic: R programming in one hour - a crash course for beginners. Here are some channels recommendations: R Programming 101 Equitable Equations Posit This book is a work in progress. If you have any feedback or spot any mistakes please enter them into this feedback survey quoting the week your comment is referring to. Thank you! :-) "],["week-7---multiple-regression.html", "Chapter 10 Week 7 - Multiple Regression 10.1 Required Packages 10.2 Importing and cleaning the data 10.3 Visualising our data 10.4 Fitting the multiple regression model 10.5 How to read a regression model 10.6 Statistical assumption checks 10.7 Sensitivity analysis 10.8 Test yourself Exercise 10.9 Video Walkthrough", " Chapter 10 Week 7 - Multiple Regression The aim of this week is to get you used to constructing multiple regression models and running statistical assumption checks in R. By this point in the semester I should have covered the theory of regression analysis in NS7151 using JASP (click here for a recap). And if I know me, in that session I likely got frustrated with some of the limitations of JASP as a program, especially around regression analysis. As such, this week is an opportunity to introduce you to some of the extra functionality of R that make it the clear choice for running regression analysis. Running a multiple regression in R is actually very easy, it's only a couple of lines of very simple code. So we'll also add in some data wrangling to give you a more realistic experience of what real world data analysis might look like. This week instead of replicating the findings of a published paper we will be using a open access large survey similar to that of your assignment data. This time the 2011 Health Survey for England. The User Guide for the data and the raw data itself can be found in this weeks folder on Moodle. 10.1 Required Packages Here are the packages that you'll need for this week. The broom package is likely new to you so you'll need to install it first. library(tidyverse) library(sjPlot) # for the correlation plots and presenting the regression model in a table library(broom) # makes it easier to work with model data 10.2 Importing and cleaning the data If you have the data file in a folder set to your working directory and you have loaded the packages, the following code will import the raw data into R. raw_data &lt;- read_csv(&quot;hse2011.csv&quot;) For this exercise we are going to try and predict Body Mass Index (BMI) using age, alcohol consumption, fruits &amp; veg consumption, and smoking behaviour. Rather than work with the whole dataset, I often find that it's easier to select out just the variables we want to use in the analysis. In our case we want variables related to BMI, alcohol, age, fruits &amp; veg, and smoking. See if you can find some of the variables that we might be able to use for this analysis. You will need to cross reference the variable names in the dataset with these topics in the User Guide. Where possible select continuous variables. The following code selects out your chosen variables (when you replace the ? with your chosen variables). The code goes on to deal with the missing data and filters the data so that we only have participants with complete data for these variables. data_cleaned &lt;- raw_data %&gt;% select(?) %&gt;% mutate(across(everything(), ~ replace(., . %in% c(-1, -8, -9, -10), NA))) %&gt;% filter(across(everything(), ~ !is.na(.))) Did you hit an error when running the code? See if you can figure out why and see if you can get to the point where we have all the variables we need for our analysis. Hint The User Guide says that one of our variables is in the dataset, but it's not there. Can we calculate it through another means? Hint2 BMI is missing, but we have weight and height. Here is the formula for calculating BMI: \\[ \\text{BMI} = \\frac{\\text{Weight in kg}}{(\\text{Height in m})^2} \\] Solution There are likely a few ways you could solve this issue, this is how I did it: data_cleaned &lt;- raw_data %&gt;% select(htval, wtval, totalwu, porfv, Age, cigst1) %&gt;% mutate(across(everything(), ~ replace(., . %in% c(-1, -8, -9, -10), NA))) %&gt;% filter(across(everything(), ~ !is.na(.))) data_cleaned &lt;- data_cleaned %&gt;% mutate(height_m = htval/100) %&gt;% mutate(bmi = wtval/(height_m^2)) The last bit of cleaning for this dataset is to convert the smoking variable into a binary variable. This will make the interpretation of it in the model a great deal easier at this stage. According to the User Guide the coding of cigst1 is as follows: 1 = Never smoked cigarettes at all 2 = Used to smoke cigarettes occasionally 3 = Used to smoke cigarettes regularly 4 = Currently cigarette smoker I suggest recoding this variable so that the first two categories are low usage and categories 3 and 4 are high usage. Also, we can create a categorical variable derived from the newly constructed BMI variable, based on the different levels of BMI. Find out the category boundaries for BMI from the literature and replace the ? in the code below. data_cleaned &lt;- data_cleaned %&gt;% mutate(cig_binary = recode(cigst1, &quot;1&quot; = &quot;Low&quot;, &quot;2&quot; = &quot;Low&quot;, &quot;3&quot; = &quot;High&quot;, &quot;4&quot; = &quot;High&quot;)) %&gt;% mutate(bmi_cat = case_when( bmi &lt; ? ~ &quot;?&quot;, bmi &gt;= ? &amp; bmi &lt; ? ~ &quot;?&quot;, bmi &gt;= ? &amp; bmi &lt; ? ~ &quot;?&quot;, bmi &gt;= ? &amp; bmi &lt; ? ~&quot;?&quot; bmi &gt;= ? ~ &quot;?&quot;)) 10.3 Visualising our data As usual, it's always a good idea to do a visual inspection of our data. Let's start with our DV. There's two ways that I can think of visualising this data. First is a histogram Click for code I decided to just use three of our BMI categories to demonstrate the most salient weight groups. Each were added manually with a coloured line and coloured text, that I placed in an empty part of the figure. As usual there's probably a more efficient way to do this but this is the way that makes sense to me, and it also makes it easier to adapt if I need this code at a later date. data_cleaned %&gt;% ggplot(aes(x = bmi)) + geom_histogram(bins = 30, alpha = 0.7, fill = &quot;darkgreen&quot;, colour = &quot;black&quot;) + geom_vline(aes(xintercept = 18.5), color = &quot;blue&quot;, linetype = &quot;dashed&quot;, size = 1) + geom_vline(aes(xintercept = 30), color = &quot;orange&quot;, linetype = &quot;dashed&quot;, size = 1) + geom_vline(aes(xintercept = 40), color = &quot;red&quot;, linetype = &quot;dashed&quot;, size = 1) + annotate(&quot;text&quot;, x = 50, y = 900, label = &quot;Underweight&quot;, hjust = 0, vjust = 1, color = &quot;blue&quot;) + annotate(&quot;text&quot;, x = 50, y = 800, label = &quot;Obese&quot;, hjust = 0, vjust = 1, color = &quot;orange&quot;) + annotate(&quot;text&quot;, x = 50, y = 700, label = &quot;Severely obese&quot;, hjust = 0, vjust = 1, color = &quot;Red&quot;) + labs(title = &quot;Figure 1: Distribution of BMI in the Sample&quot;, x = &quot;Body Mass Index (BMI)&quot;, y = &quot;Frequency&quot;) + theme_minimal() Another way we could visualise this variable is in the same way that we visualised the blood pressure data in week 4. See if you can adapt that code to make the following figure (or something close to it). The figure certainly looks pretty, but perhaps it's not overly informative. We should also check the predictor variables as well. As they are all continuous variables we can create the plots for these all together just as we did in week 5 with the piviot and facet_wrap. Click for code You can of course run these as three separate histograms if you find it easier. data_cleaned %&gt;% select(totalwu, Age, porfv)%&gt;% rename(Alcohol = totalwu, `Fruit&amp;Veg` = porfv) %&gt;% pivot_longer(cols = c(Alcohol, Age, `Fruit&amp;Veg`), names_to = &quot;Variable&quot;, values_to = &quot;Value&quot;)%&gt;% ggplot(aes(x= Value)) + geom_histogram(fill = &quot;darkred&quot;, colour = &quot;black&quot;)+ facet_wrap(~Variable, scale = &quot;free&quot;) + labs(title = &quot;Figure 3: Histograms of continuious predictor variables&quot;, y = &quot;Frequency&quot;) It looks like the alcohol and fruit &amp; veg variables are both right skewed. This is not necessary a violation of our statistical assumption for regression models, but we very likely have some outliers. We could consider removing these if our statistical assumptions for the model are violated. 10.4 Fitting the multiple regression model As I mentioned up top, fitting a regression model in R is very simple. Here is the code for our model first model that predicts BMI using alcohol consumption, age, portions of fruit and veg and smoking behaviour: model &lt;- lm(bmi ~ totalwu + Age + porfv + cig_binary, data = data_cleaned) It uses the lm function which in this context stands for linear model. We add the bmi variable as our dependent variable, and each variable after the ~ are our predictor variables. It is then important to include the data = argument as, sadly, we can't pipe the data in as we often do. Also, this needs to go at the end of the expression rather than the beginning (an aspect that often trips me up). The lm function creates a list object (instead of a dataframe or tibble as you're often used to seeing). These are not easy to read (click on the model object you have just created to see for yourself). So instead, to get the results of the model you can run the following to print the results in the console: summary(model) Or, if you have the broom package loaded you can add the tidy function to the model expression to store the model as a tibble. library(broom) model_tibble &lt;- lm(bmi ~ totalwu + Age + porfv + cig_binary, data = data_cleaned) %&gt;% tidy() Both of these ways of presenting our model are of course very ugly and won't do for communicating our findings. The sjPlots package is the best package that I've found for putting our regression findings into APA style tables. Make sure the package is loaded and run the following to get the results in a nice table, fit for exporting. library(sjPlot) tab_model(model) As with most functions there are some arguments we can use to improve this table somewhat. Here is the code for the final table. tab_model(model, pred.labels = c(&quot;(Intercept)&quot;, &quot;Alcohol&quot;, &quot;Age&quot;, &quot;Portions of Fruit &amp; Veg&quot;, &quot;Smoker(ref=high)&quot;), dv.labels = c(&quot;BMI&quot;), string.est = &quot;B&quot;, title = &quot;Table 1. Predictors of BMI (Model 1)&quot;) 10.5 How to read a regression model Make sure to check back to my NS7151 content for a full explanation of how to interpret a regression analysis, however, here are the main takeaways: R-Squared and Adjusted R-Squared: The \\(R^2\\) value in our model is 0.031, indicating that 3.1% of the variance in BMI is explained by our predictors. It's essential to look at the adjusted \\(R^2\\) for multiple regression, which penalises the model for including non-useful predictors. Estimates (Coefficients): Continuous Variables - The coefficient for Age is 0.051. This means that for each additional year of life, BMI is expected to increase by 0.051 points, assuming all other variables are held constant. Categorical Variables - The coefficient for cig_binary is -0.22. Having 'low' cigarette usage is associated with a 0.22-point decrease in BMI compared to having 'high' usage. This interpretation can also be flipped given the negative sign. Just to be on the safe side it might make sense for us to change the reference category for this variable, rather than remembering to flip the sign during the write up. This amendment only changes the interpretation of the model and not this fit. We can do that with the following code: data_cleaned$cig_binary &lt;- as.factor(data_cleaned$cig_binary) data_cleaned$cig_binary &lt;- relevel(data_cleaned$cig_binary, ref = &quot;Low&quot;) model &lt;- lm(bmi ~ totalwu + Age + porfv + cig_binary, data = data_cleaned) summary(model) Confidence Intervals: A 95% Confidence Interval (CI) gives us a range in which we can be 95% confident that the population parameter lies. Narrower CIs indicate more reliable estimates. If the CI for a coefficient does not include zero, it's generally considered statistically significant. P-Value: The p-value indicates the probability that the observed effect occurred by chance if there were actually no effect. A p-value less than 0.05 is generally considered statistically significant, although this threshold can vary. Standard Error: Standard error measures the amount of variability in the estimate for the coefficient. Lower standard error values imply more precise estimates. 10.5.1 Regression Equation To summarise the linear model fitted to predict bmi, the regression equation is as follows: \\[ \\text{BMI} = 25.29 + 0.05 \\times \\text{Age} - 0.05 \\times \\text{PorFV} + \\epsilon \\] 10.6 Statistical assumption checks I wanted to explain the above modelling aspect prior to this section on statistical assumption checks (as its likely the main reason why you're here), but in reality these assumption checks either take place prior to, or during, the model fitting process. The assumptions we'll check for this example are as followed: - Linearity: Predictor variables should have a linear relationship with the dependent variable. - Multicollinearity: Predictor variables should not share particularly strong relationships with each other. - Normality of residuals: The residuals—the difference between the observed and predicted values—should be approximately normally distributed. - Homoscedasticity: The data should be homoscedastic, meaning the variance of the errors should remain constant across levels of the independent variable(s). More assumptions exist, but this should be sufficient for this level of your study. 10.6.1 Linear relationship with the DV To check to see if we have a linear relationship with the DV we can use our pivot_longer trick to run all three scatter plots simultaneously for the three continuous variables in our model. Take and adapt the histogram code from above and try to make the image below for yourself. For Age we have evidence of a nice liner trend with BMI. For the Alcohol and Fruit&amp;Veg it is hard to tell. This is perhaps and argument to check again later if we decide to remove those outliers. 10.6.2 Check for Multicollinearity Next, we need to check the relationships between our predictor variables. Here we are looking for the correlation coefficient of each relationship to be no more than around r = 0.7. Adapt the code from week 5 to make the following correlation matrix and correlation plot. This looks good to me. The greatest magnitude correlation coefficient between our predictor variables is that of Age and Portions of fruit &amp; Veg at r = 0.086. Far below our multicollinearity cut off of 0.7. Next week I'll show you another check (VIF) which is more intuitive for use with categorical variables. 10.6.3 Check the residuals (normality) Our normality check for regression is not the normality of the continuous variables themselves but the residuals of the model. For a reminder on residuals see the content I shared with you in NS7151(Link). Unlike when using JASP, in R we can extract the residuals and fitted values from our model. Which we can then use to manually create the assumption check visualisations. The code below extracts these values from the model I have created. residuals_data &lt;- tibble(model_residuals = residuals(model), model_fitted_values = fitted(model)) Once the residuals are extracted from the model it's just a matter of running a histogram. In the image below I've run this and added a normal distribution curve to compare our distribution to, and a bar along the bottom to show where some of our harder to spot data points are. Click for code By adding aes(y = ..density.. to geom_histogram and the stat_function expression we can compare our data to that of a perfect normal distribution. The geom_rug is a handy addition that allows us to see where the data points are on plots where were dealing with a lot of data. residuals_data %&gt;% ggplot(aes(x = model_residuals)) + geom_histogram(aes(y = ..density..), bins = 50, colour = &quot;black&quot;, fill = &quot;darkblue&quot;) + stat_function(fun = dnorm, args = list(mean = mean(residuals_data$model_residuals, na.rm = TRUE), sd = sd(residuals_data$model_residuals, na.rm = TRUE)), colour = &quot;red&quot;, size = 1.5, alpha = 0.7) + geom_rug() + labs( title = &quot;Figure 5. Histogram of Residuals (model 1)&quot;, x = &quot;Residuals&quot;, y = &quot;Density&quot; ) + theme_minimal() 10.6.4 Check for homoscedasticity Homoscedasticity implies that the variance of the error terms (the residuals) is constant across the independent variables. In other words, the \"spread\" of the residuals should remain consistent as the fitted value changes. Violations of this assumption, known as heteroscedasticity, can lead to inefficient parameter estimates and unreliable significance tests. To visually inspect for homoscedasticity, we often use a plot of residuals against the fitted values (values that your model predicts). A \"well-behaved\" plot would show a random scattering of points, which would indicate heteroscedasticity. A smooth fitted line of best fit (a loess curve) should be a flat line at zero. Below is the R code that generates such a plot: residuals_data %&gt;% ggplot(aes(x = model_fitted_values, y = model_residuals)) + geom_point(alpha = 0.5) + geom_smooth(method = &quot;loess&quot;, se = FALSE, colour = &quot;red&quot;) + labs( title = &quot;Residuals vs Fitted Values&quot;, x = &quot;Fitted Values&quot;, y = &quot;Residuals&quot; ) + theme_minimal() And hmmm, it looks like we have an issue here. That's a pain, it's likely caused by our outliers so it's probably worth conducting a sensitivity analysis. 10.7 Sensitivity analysis Sensitivity analysis is a technique employed to assess how robust your model's results are to changes in data or assumptions. In the context of regression modelling, it often involves running multiple versions of the same model to see how certain modifications affect the outcomes. As with our current analysis we have run the model with outliers included, now lets run it again with outliers removed. The following code calculates the outliers based on the upper and lower quartile + or - 1.5 time the interquartile range. Here is an explanation of this technique (Link), we also used this briefly with labelling out outliers in a boxplot in week 2. sensitivity_model_data &lt;- data_cleaned %&gt;% mutate(iqr_totalwu = IQR(totalwu, na.rm = TRUE), iqr_porfv = IQR(porfv, na.rm = TRUE), upper_totalwu = quantile(totalwu, 0.75, na.rm = TRUE) + 1.5 * iqr_totalwu, lower_totalwu = quantile(totalwu, 0.25, na.rm = TRUE) - 1.5 * iqr_totalwu, upper_porfv = quantile(porfv, 0.75, na.rm = TRUE) + 1.5 * iqr_porfv, lower_porfv = quantile(porfv, 0.25, na.rm = TRUE) - 1.5 * iqr_porfv) %&gt;% filter(totalwu &lt; upper_totalwu &amp; totalwu &gt; lower_totalwu, porfv &lt; upper_porfv &amp; porfv &gt; lower_porfv)%&gt;% select(totalwu, Age, porfv, cig_binary, bmi) 10.8 Test yourself Exercise Run the model and the assumption checks again for the sensitivity_model_data. If you name your model sensitivity_model then the below code will allow you to combine both of the models together into the same table. Don't forget to also change the reference category for the smoking variable (or at least keep them consistant across the models). tab_model(model, sensitivity_model, pred.labels = c(&quot;(Intercept)&quot;, &quot;Alcohol&quot;, &quot;Age&quot;, &quot;Portions of Fruit &amp; Veg&quot;, &quot;Smoker(ref=Low)&quot;), dv.labels = c(&quot;Model&quot;, &quot;Sensitivity model&quot;), string.est = &quot;B&quot;, title = &quot;Table 3. Predictors of BMI (Model &amp; Sensitivity model)&quot;) 10.8.1 What does the sensitivity analysis tell us? 10.8.1.1 Sensitivity assumption checks From your assumption checks on the sensitivity analysis data, you might have noticed a slight difference in the homoscedasticity plot, but the data still violates the assumption (in my opinion). If you ran the histograms again you will have seen that the alcohol variable is still right skewed. At this point in the analysis I took the log of the alcohol variable, to see if that solves the issue. It didn't and anyway this would in-turn make it harder to in interpret the findings so perhaps the two models we have might be the best we can do with this data. If I we're to continue this research, I would also want to read further into the alcohol research literature and see how they most often use such a variable. There is often added complexity with asking people about alcohol consumption. 0 alcohol intake can be explained by a respondent being under 18, a non-drinker adult, or an ex-alcoholic that now abstains, all of which are likely to differ greatly in other related health statuses. Either way it is important to report this violation and present both models so that the reader can make their own interpretation. 10.8.1.2 Sensitivity model results Here are the results you will hopefully end up with: Interpretation of sensitivity results Alcohol: for every one-point increase on the alcohol variable, BMI decreases by 0.02 points. Age: for every one-year increase in age BMI increases by 0.05 points Fruit &amp; Veg: for every one-point increase on the fruit &amp; veg variable, BMI decreases by -0.08 points Smoking: being a high smoker (as compared to low) is associated with a 0.082 point increase in BMI From this comparison hopefully you can see how fickle p-values can be. Just a small change in the data and the findings change completely. This is why it's important to look at effect sizes and decide for yourself what is meaningful. Personally, it seems to me that like Age is the most meaningful predictor of BMI in this model. Just see what happens to the \\(R^2\\) value when you run the model with and without the age variable. If you're surprised by the lack of influence of any of these variables, it's worth remembering just how complicated understanding the predictors of weight can be. 10.9 Video Walkthrough This book is a work in progress. If you have any feedback or spot any mistakes please enter them into this feedback survey quoting the week your comment is referring to. Thank you! :-) "],["week-8---logistic-regression.html", "Chapter 11 Week 8 - Logistic regression 11.1 The Data 11.2 Data wrangling 11.3 Recreating Table 1 11.4 Running and interpreting a logistic regression 11.5 Test yourself", " Chapter 11 Week 8 - Logistic regression This week we'll be replicating the analysis related to a recent paper by Jamil et al (2023) titled Socio-demographic determinants of Monkeypox virus preventive behaviour: A cross-sectional study in Pakistan. The study uses a frequent framing in Public Health; surveying on knowledge, attitudes and perceptions (KAP). Researcher often uses KAP studies to identify knowledge gaps, misconceptions and attitudes that may influence preventative practices. With Monkeypox being declared a public health emergency of international concern in mid 2022 this is kind of study is an understandable first step in a public health response. The authors conduct a series of logistic regressions and have shared their data. Making this good candidate for us to practice logistic regression prior to your potentially using such an analysis in your assignment. This weeks code replicates Table 1 and last three columns of Table 5 from the paper. 11.1 The Data The authors shared the data as an SPSS file. Fiddling around with the SPSS file to get it into a format to use in this analysis is a useful skill but not one related to your assignment. So, instead, I've given you a somewhat cleaned up version of the dataset as a .csv file. Download the file Jamil et al 2023 cleaned.csv from Moodle. Set your working directory and load the data into RStudio. jamil2023 &lt;- read_csv(&quot;Jamil et al 2023 cleaned.csv&quot;) Take a look at the data file and take a look at Table 1 from Jamil et al. See if you can identify which variables we need to recreate that table. From the looking at the dataset, the age variable will need to be converted into a categorical variable. See if you can do this without looking at the code in the blue box below. You'll need to use the case_when function. I've given you example of how to use this function previously. Check the paper paper for the categories that the authors used and make sure to call your variable age_cat so that the rest of the code in this chapter works. Solution jamil2023 &lt;- jamil2023 %&gt;% mutate(age_cat = case_when( age &lt;= 20 ~ &quot;less than 20&quot;, age &gt; 20 &amp; age &lt; 31 ~ &quot;21 - 30&quot;, age &gt;= 31 &amp; age &lt;= 40 ~ &quot;31 - 40&quot;, age &gt; 40 ~ &quot;40+&quot;, TRUE ~ &quot;Other&quot;)) Run the following code to get a count for each of the age groups. You may need to replace age_cat with your own variable name if you mutated this variable without my code. jamil2023 %&gt;% group_by(age_cat) %&gt;% summarise(n = n()) Do our numbers match that of Table 1 in the paper? No, it doesn't look like they do. Does that mean we've recoded the variable incorrectly? No, I don't think that's it either. Then why do we have different numbers to them? Well, for after a lot of head scratching, I found that their 21-30 category is actually 21-29 and their 31-40 category is actually 29-39. The code below creates their calculation and adds it to the dataset. jamil2023 &lt;- jamil2023 %&gt;% mutate(age_cat_author = case_when( age &lt;= 20 ~ &quot;less than 20&quot;, age &gt; 20 &amp; age &lt; 29 ~ &quot;21 - 30&quot;, age &gt;= 29 &amp; age &lt; 40 ~ &quot;31 - 40&quot;, age &gt;= 40 ~ &quot;40+&quot;, TRUE ~ &quot;Other&quot;)) jamil2023 %&gt;% group_by(age_cat_author) %&gt;% summarise(n = n()) Very strange way to code a variable. I wonder if this will prove to be an important difference later in the analysis. 11.2 Data wrangling The following code chunk gets the data in the right form for the table and the later regression all in one go. The code: 1. Renames the variables so they look appropriate on the tables. 2. Changes the data type of each variable to a factor (rather than just character data) 3. Changes the order for each variable that needs its order changed. 4. Dummy codes the the dependent variables ready for the logistic regression. What is dummy coding? Dummy coding is used in regression analysis to transform a categorical variable into a binary variable (0s and 1s). This is necessary because regression models require numerical input. In dummy coding, one category is chosen as a reference group and is assigned a value of 0. The other categories are then compared against this reference group. In this case, we want to take the continuous knowledge variable and use that in our model so that \"Good Knowledge\" is given the value of 1 and \"Poor Knowledge\" is given the value of 0. analysis_data &lt;- jamil2023 %&gt;% mutate(Gender = factor(gender, levels = c(&quot;Male&quot;, &quot;Female&quot;)), Age = factor(age_cat_author, levels = c(&quot;less than 20&quot;, &quot;21 - 30&quot;, &quot;31 - 40&quot;, &quot;40+&quot;)), `Marital status` = factor(marital_status, levels = c(&quot;Married&quot;, &quot;Unmarried&quot;)), Residence = factor(residence, levels = c(&quot;Urban&quot;, &quot;Rural&quot;)), Education = factor(education, levels = c(&quot;Primary up to Matriculation&quot;, &quot;Intermediate&quot;, &quot;Graduation&quot;, &quot;Post-graduation&quot;)), `Monthly income in PKR` = factor(income, levels = c(&quot;&lt;50,000&quot;, &quot;50,000 - 100,000&quot;, &quot;&gt;100,000&quot;))) %&gt;% mutate(knowledge_dummy = recode(knowledge_cat, &quot;Poor knowledge&quot; = 0, &quot;Good knowledge&quot; = 1), attitude_dummy = recode(attitude_cat, &quot;Negative attitude&quot; = 0, &quot;Positive attitude&quot; = 1), practice_dummy = recode(practice_cat, &quot;Not positive practice&quot; = 0, &quot;Positive practice&quot; = 1)) Remember, if you see a big chunk of code like this and you would like it explained to you, this is an excellent way to use an AI. Here's what Chat GPT says about the above code: Link 11.3 Recreating Table 1 The gtsummary package is really useful for creating a descriptive statistics table such as Table 1 in Jamil et al (2023). First, make sure the package is installed and loaded and then all you need to do is select the variables you need to summaries and use the tbl_summary() function. library(gtsummary) table1_data &lt;- analysis_data %&gt;% select(Gender, Age, `Marital status`, Residence, Education, `Monthly income in PKR`) tbl_summary(table1_data) That last line should create the table in your Viewer window. If you want to see the table in its full glory, click the \"Show in new window\" button and that will open the table through your web browser. This isn't quite the same as the authors version, but we can tidy it up a little by playing around with some of the arguments by converting this in to a table that is compatible with the gt package. Here is a video explaining the gt package and what its possible to build with it. If you save the web browser version and open it in Microsoft Word you can also edit the table. Really useful for adding your final last few touches. library(gt) table1_raw &lt;- tbl_summary(table1_data) %&gt;% as_gt() table1_raw %&gt;% tab_header(&quot;Table 1. Study sample socio-demographic characteristics (N = 1041)&quot;) %&gt;% tab_style(style = cell_text(align = &quot;left&quot;, weight = &quot;bold&quot;), locations = cells_title(groups = &quot;title&quot;)) %&gt;% cols_label(label = &quot;Variables&quot;, stat_0 = &quot;Frequency&quot;) %&gt;% tab_footnote(&quot;PKR: Pakistani Rupee&quot;) 11.4 Running and interpreting a logistic regression In table 5 and 6 the author runs a series of univariate logistic regression (i.e. a simple regression with just the one variable) and then a multivariate logistic regression with any univariate analysis with a p-value less than 0.25. Normally the cut off for this sort of procedure is 0.1. However this would have just left two variables for the model so I can see why they chose to pick 0.25. An important step of conducting a logistic regression is to carefully consider the category that you would like all of the other categories to be compared to. The following code tells out data which categories we want for a reference category and then performs the logistic regression. The tab_model function from the sjPlot package is likely the best and easiest way to present the findings of a logistic regression in a good looking table. analysis_data &lt;- analysis_data %&gt;% mutate(Education = relevel(as.factor(Education), ref = &quot;Intermediate&quot;)) univariate_ed &lt;- glm(knowledge_dummy ~ Education, data = analysis_data, family = binomial) tab_model(univariate_ed, show.reflvl = TRUE, show.intercept = FALSE) Run the code again but with ref = \"Graduation\" to see how this changes the reference category. The underlying results are still the same but now each group is in comparison to the Graduation group as opposed to the Intermediate group. To run the full multivariate model all we need to do is build out the code so that each reference category is set and the model contains all the relevant variables. analysis_data &lt;- analysis_data %&gt;% mutate(Gender = relevel(as.factor(Gender), ref = &quot;Female&quot;), Age = relevel(as.factor(Age), ref = &quot;40+&quot;), age_cat = relevel(as.factor(age_cat), ref = &quot;40+&quot;), `Marital status` = relevel(as.factor(`Marital status`), ref = &quot;Unmarried&quot;), Residence = relevel(as.factor(Residence), ref = &quot;Urban&quot;), Education = relevel(as.factor(Education), ref = &quot;Intermediate&quot;), `Monthly income in PKR` = relevel(as.factor(`Monthly income in PKR`), ref = &quot;&gt;100,000&quot;)) multivariate_model &lt;- glm(knowledge_dummy ~ Gender + Age + Residence + Education, data = analysis_data, family = binomial) tab_model(multivariate_model, show.reflvl = TRUE, show.intercept = FALSE, dv.labels = &quot;&quot;, title = &quot;Model 1: Predictors of MPox knowledge (author age coded)&quot;) multivariate_model &lt;- glm(knowledge_dummy ~ Gender + Age + Residence + Education, data = analysis_data, family = binomial) tab_model(multivariate_model, show.reflvl = TRUE, show.intercept = FALSE, dv.labels = &quot;&quot;, title = &quot;Model 1: Predictors of MPox knowledge (author age coded)&quot;) Remember earlier when we coded the age variable and found that the authors misscoded in their initial attempt. The code above sets the reference catagory for the variable that we calculated (which I called age_cat) see if you can rerun the mode with that variable. Which variables are significant in the authors model and which are significant when the age variable is coded correctly. Solution multivariate_model2 &lt;- glm(knowledge_dummy ~ Gender + age_cat + Residence + Education, data = analysis_data, family = binomial) tab_model(multivariate_model2, show.reflvl = TRUE, show.intercept = FALSE, dv.labels = &quot;&quot;, title = &quot;Model 2: Predictors of MPox knowledge (corrected age coding)&quot;) 11.5 Test yourself Run an additional model to see what the findings would have been if all of the variables were included in the analysis. "],["week-9-assignment-data-details.html", "Chapter 12 Week 9 Assignment data details 12.1 Read in the data and codebook 12.2 Video walkthroughs 12.3 Samples code for you to adapt: 12.4 Suggested workflow", " Chapter 12 Week 9 Assignment data details This week, and for the rest of this module now, we'll be working on the Scottish Health Survey 2021 data to set you up for your chosen analysis for this modules final assessment. 12.1 Read in the data and codebook By now I likely don't need to explain how to read data into R, but seeing as it's essential for the assignment it's probably best to go over best practices. As you'll likely be coming back to you analysis script multiple times I suggest creating a folder with three files. 1. A R script (or RMarkdown script, more on this next week), 2. the data file, 3. the codebook. It might also make good sense to include any of the pdfs related to the dataset, just so you know where to find them. Open you script file and set your working directory. You can do this by clicking Session -&gt; Set Working Directory -&gt; Choose Directory or To Source File Location. Alternatively, just to avoid doing this every time you open your script, you can do this with code with the setwd function. Also it makes sense to include all packages you use in your script at the top of the page. setwd(&quot;your_directory_path_here&quot;) library(&quot;packagename1&quot;) library(&quot;packagename2&quot;) library(&quot;packagename3&quot;) library(&quot;packagename~&quot;) raw_data &lt;- read_csv(&quot;Suggested SHES21 Dataset.csv&quot;) codebook &lt;- read_csv(&quot;Codebook.csv&quot;) I have been kind and cut down of the over 2000 variables in the original dataset and given you a far more manageable 66 variables. Many of these variables will still take some data wrangling to get into a usable format for the assignment analysis. To help you out, I've include some modifiable code chunks that you should be able to use to achieve this. The codebook document gives you a brief description of the variables (for a full account of the questions asked see the associated questionnaire materials). Give me all the data, I'm ready for it! So, you want to full experience of analysis this dataset? Excellent! The original file actually came to me in SPSS format. As such, it takes a little playing with to get it into R. The haven package is useful for this. I'd suggest saving file out as a csv file and then continue with the analysis from that file. There will be around 2000 variables for you to hack your way through, but perhaps you'll be able to spot some interesting variable that I missed. Some questions were only asked to children so this could be an good angle that my selection dataset does not cover. Good luck! 12.2 Video walkthroughs 12.2.1 Multiple regression This is a video of me setting up the data (which you'll need no matter which analysis you'll conduct) and then conducting a multiple regression. 12.2.2 Logistic regression In this video I conduct a logistic regression. 12.3 Samples code for you to adapt: 12.3.1 Dealing with missing data This code should be adequate for labeling all of your missing data with real NA's. missing_values &lt;- c(&quot;Unclassifiable&quot;, &quot;Refused&quot;, &quot;Don&#39;t know&quot;, &quot;Schedule not obtained&quot;, &quot;Schedule not applicable&quot;, &quot;Not applicable&quot;, &quot;Reading not obtained&quot;) data_cleaned &lt;- data %&gt;% mutate_all(~replace(., . %in% missing_values, NA)) 12.3.2 Checking missing data The naniar package is useful for checking missing data in large data sets. Install the package and run the following code to get plots that summarise the missing data library(naniar) gg_miss_var(cleaned_data) vis_miss(cleaned_data) 12.3.3 Summary of catagorical variables This code summarises a single categorical variable data_cleaned %&gt;% group_by(variable) %&gt;% summarise(n = n()) 12.3.4 Summary of numerical variables data_cleaned %&gt;% summarise(average_x = mean(variable)) 12.3.5 Converting variable from character to numerical Due to the missing data being initially coded as words (e.g. \"Unclassifiable\", \"Refused\", etc) the date is imported as a character variable, even if the data in the variable are actually numbers. Run the code below after replacing the missing data to get your numerical variables ready for analysis. data_cleaned &lt;- data %&gt;% mutate(variable = as.numeric(variable)) 12.3.6 Recode a catagorical variable into different groupings This code takes a variable and computes a new variable based on your specification. Useful for if you want to dichotomous a catagorical variable with more than two options. data_cleaned &lt;- data %&gt;% mutate(recoded_var = case_when( original_var %in% c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) ~ &quot;Category1&quot;, original_var %in% c(&quot;D&quot;, &quot;E&quot;, &quot;F&quot;) ~ &quot;Category2&quot;, TRUE ~ &quot;Other&quot;)) 12.3.7 Reverse code a numerical likert scale An easy way to reverse code a variable is to take the original value away from one more than the top of your scale. For example if you have a five point Likert scale. The calculation would be 6 - [each value]. And for a 7 point Likert scale it would be 8 - [each value] data_cleaned &lt;- data %&gt;% mutate(var_5_point_reversed = 6 - var_5_point, var_7_point_reversed = 8 - var_7_point, var_10_point_reversed = 11 - var_10_point) 12.3.8 Coding a psychometric scale into a single variable The first expression here is for use with numerical data. The second expression takes a likert scale expressed in words and converts it to numbers. recoded_data &lt;- recoded_data %&gt;% mutate(scale_score = mean(c_across(c(&quot;item1&quot;, &quot;item2&quot;)), na.rm = TRUE)) #note you may need to convert the variable to numerical first recoded_data &lt;- data %&gt;% mutate(variable1 = recode(variable1, &quot;Not at all&quot; = 0, &quot;No more than usual&quot; = 1, &quot;Rather more than usual&quot; = 2, &quot;Much more than usual&quot; = 3))%&gt;% mutate(variable2 = recode(variable2, &quot;Not at all&quot; = 0, &quot;No more than usual&quot; = 1, &quot;Rather more than usual&quot; = 2, &quot;Much more than usual&quot; = 3)) 12.4 Suggested workflow I suggest having a cleaning and wrangling script that ends with you writing your final data object as a csv file with the following code: write_csv(data, &quot;name for your new csv file.csv&quot;) #the .csv is important to include. Then start a fresh R script or RMarkdown file for the analysis. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
